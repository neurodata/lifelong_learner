{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from itertools import product\n",
    "from math import log2\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "#Infrastructure\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import NotFittedError\n",
    "\n",
    "#Data Handling\n",
    "from sklearn.utils.validation import (\n",
    "    check_X_y,\n",
    "    check_array,\n",
    "    NotFittedError,\n",
    ")\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "\n",
    "#Utils\n",
    "import numpy as np\n",
    "from sklearn.base import clone \n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def homogenize_labels(a):\n",
    "    u = np.unique(a)\n",
    "    return np.array([np.where(u == i)[0][0] for i in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _finite_sample_correction(posteriors, num_points_in_partition, num_classes):\n",
    "    '''\n",
    "    encourage posteriors to approach uniform when there is low data\n",
    "    '''\n",
    "    correction_constant = 1 / (num_classes * num_points_in_partition)\n",
    "\n",
    "    zero_posterior_idxs = np.where(posteriors == 0)[0]\n",
    "\n",
    "    c = len(zero_posterior_idxs) / (num_classes * num_points_in_partition)\n",
    "    posteriors *= (1 - c)\n",
    "    posteriors[zero_posterior_idxs] = correction_constant\n",
    "    return posteriors\n",
    "\n",
    "class UncertaintyForest(BaseEstimator, ClassifierMixin):\n",
    "    '''\n",
    "    based off of https://arxiv.org/pdf/1907.00325.pdf\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_depth=30,\n",
    "        min_samples_leaf=1,\n",
    "        max_samples = 0.32,\n",
    "        max_features_tree = \"auto\",\n",
    "        n_estimators=200,\n",
    "        bootstrap=False,\n",
    "        parallel=True):\n",
    "\n",
    "        #Tree parameters.\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features_tree = max_features_tree\n",
    "\n",
    "        #Bag parameters\n",
    "        self.n_estimators = n_estimators\n",
    "        self.bootstrap = bootstrap\n",
    "        self.max_samples = max_samples\n",
    "\n",
    "        #Model parameters.\n",
    "        self.parallel = parallel\n",
    "        self.fitted = False\n",
    "\n",
    "    def _check_fit(self):\n",
    "        '''\n",
    "        raise a NotFittedError if the model isn't fit\n",
    "        '''\n",
    "        if not self.fitted:\n",
    "                msg = (\n",
    "                        \"This %(name)s instance is not fitted yet. Call 'fit' with \"\n",
    "                        \"appropriate arguments before using this estimator.\"\n",
    "                )\n",
    "                raise NotFittedError(msg % {\"name\": type(self).__name__})\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        get the estimated posteriors across trees\n",
    "        '''\n",
    "        X = check_array(X)\n",
    "                \n",
    "        def worker(tree_idx, tree):\n",
    "            #get the nodes of X\n",
    "            # Drop each estimation example down the tree, and record its 'y' value.\n",
    "            return tree.apply(X)\n",
    "            \n",
    "\n",
    "        if self.parallel:\n",
    "            return np.array(\n",
    "                    Parallel(n_jobs=-1)(\n",
    "                            delayed(worker)(tree_idx, tree) for tree_idx, tree in enumerate(self.ensemble.estimators_)\n",
    "                    )\n",
    "            )         \n",
    "        else:\n",
    "            return np.array(\n",
    "                    [worker(tree_idx, tree) for tree_idx, tree in enumerate(self.ensemble.estimators_)]\n",
    "                    )\n",
    "        \n",
    "    def get_transformer(self):\n",
    "        return lambda X : self.transform(X)\n",
    "        \n",
    "    def vote(self, nodes_across_trees):\n",
    "        return self.voter.predict(nodes_across_trees)\n",
    "        \n",
    "    def get_voter(self):\n",
    "        return self.voter\n",
    "        \n",
    "                        \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        #format X and y\n",
    "        X, y = check_X_y(X, y)\n",
    "        check_classification_targets(y)\n",
    "        self.classes_, y = np.unique(y, return_inverse=True)\n",
    "        \n",
    "        #define the ensemble\n",
    "        self.ensemble = BaggingClassifier(\n",
    "            DecisionTreeClassifier(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                max_features=self.max_features_tree\n",
    "            ),\n",
    "            n_estimators=self.n_estimators,\n",
    "            max_samples=self.max_samples,\n",
    "            bootstrap=self.bootstrap,\n",
    "            n_jobs = -1\n",
    "        )\n",
    "        \n",
    "        #fit the ensemble\n",
    "        self.ensemble.fit(X, y)\n",
    "        \n",
    "        class Voter(BaseEstimator):\n",
    "            def __init__(self, estimators_samples_, classes, parallel = True):\n",
    "                self.n_estimators = len(estimators_samples_)\n",
    "                self.classes_ = classes\n",
    "                self.parallel = parallel\n",
    "                self.estimators_samples_ = estimators_samples_\n",
    "            \n",
    "            def fit(self, nodes_across_trees, y, fitting = False):\n",
    "                self.tree_idx_to_node_ids_to_posterior_map = {}\n",
    "\n",
    "                def worker(tree_idx):\n",
    "                    nodes = nodes_across_trees[tree_idx]\n",
    "                    cal_nodes = nodes[self.estimators_samples_[tree_idx]] if fitting else nodes\n",
    "                    y_cal = y[self.estimators_samples_[tree_idx]] if fitting else y                    \n",
    "                    \n",
    "                    #create a map from the unique node ids to their classwise posteriors\n",
    "                    node_ids_to_posterior_map = {}\n",
    "\n",
    "                    #fill in the posteriors \n",
    "                    for node_id in np.unique(cal_nodes):\n",
    "                        cal_idxs_of_node_id = np.where(cal_nodes == node_id)[0]\n",
    "                        cal_ys_of_node = y_cal[cal_idxs_of_node_id]\n",
    "                        class_counts = [len(np.where(cal_ys_of_node == y)[0]) for y in np.unique(y) ]\n",
    "                        posteriors = np.nan_to_num(np.array(class_counts) / np.sum(class_counts))\n",
    "\n",
    "                        #finite sample correction\n",
    "                        posteriors_corrected = _finite_sample_correction(posteriors, len(cal_idxs_of_node_id), len(self.classes_))\n",
    "                        node_ids_to_posterior_map[node_id] = posteriors_corrected\n",
    "\n",
    "                    #add the node_ids_to_posterior_map to the overall tree_idx map \n",
    "                    self.tree_idx_to_node_ids_to_posterior_map[tree_idx] = node_ids_to_posterior_map\n",
    "                    \n",
    "                for tree_idx in range(self.n_estimators):\n",
    "                        worker(tree_idx)\n",
    "                return self\n",
    "                        \n",
    "                        \n",
    "            def predict_proba(self, nodes_across_trees):\n",
    "                def worker(tree_idx):\n",
    "                    #get the node_ids_to_posterior_map for this tree\n",
    "                    node_ids_to_posterior_map = self.tree_idx_to_node_ids_to_posterior_map[tree_idx]\n",
    "\n",
    "                    #get the nodes of X\n",
    "                    nodes = nodes_across_trees[tree_idx]\n",
    "\n",
    "                    posteriors = []\n",
    "                    node_ids = node_ids_to_posterior_map.keys()\n",
    "\n",
    "                    #loop over nodes of X\n",
    "                    for node in nodes:\n",
    "                        #if we've seen this node before, simply get the posterior\n",
    "                        if node in node_ids:\n",
    "                            posteriors.append(node_ids_to_posterior_map[node])\n",
    "                        #if we haven't seen this node before, simply use the uniform posterior \n",
    "                        else:\n",
    "                            posteriors.append(np.ones((len(np.unique(self.classes_)))) / len(self.classes_))\n",
    "                    return posteriors\n",
    "\n",
    "                if self.parallel:\n",
    "                    return np.mean(\n",
    "                            Parallel(n_jobs=-1)(\n",
    "                                    delayed(worker)(tree_idx) for tree_idx in range(self.n_estimators)\n",
    "                            ), axis = 0\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    return np.mean(\n",
    "                            [worker(tree_idx) for tree_idx in range(self.n_estimators)], axis = 0)\n",
    "                \n",
    "        #get the nodes of the calibration set\n",
    "        nodes_across_trees = self.transform(X) \n",
    "        self.voter = Voter(estimators_samples_ = self.ensemble.estimators_samples_, classes = self.classes_, parallel = self.parallel)\n",
    "        self.voter.fit(nodes_across_trees, y, fitting = True)\n",
    "        self.fitted = True\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.classes_[np.argmax(self.predict_proba(X), axis=-1)]\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.voter.predict_proba(self.transform(X))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LifeLongForest():\n",
    "    def __init__(self, acorn = None, verbose = False, model = \"uf\"):\n",
    "        self.X_across_tasks = []\n",
    "        self.y_across_tasks = []\n",
    "        \n",
    "        self.transformers_across_tasks = []\n",
    "        \n",
    "        #element [i, j] votes on decider from task i under representation from task j\n",
    "        self.voters_across_tasks_matrix = []\n",
    "        self.n_tasks = 0\n",
    "        \n",
    "        self.classes_across_tasks = []\n",
    "        \n",
    "        if acorn is not None:\n",
    "            np.random.seed(acorn)\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "    def check_task_idx_(self, task_idx):\n",
    "        if task_idx >= self.n_tasks:\n",
    "            raise Exception(\"Invalid Task IDX\")\n",
    "    \n",
    "    def new_forest(self, \n",
    "                   X, \n",
    "                   y, \n",
    "                   epochs = 100, \n",
    "                   lr = 5e-4, \n",
    "                   n_estimators = 100, \n",
    "                   max_samples = .63,\n",
    "                   bootstrap = False,\n",
    "                   max_depth = 30,\n",
    "                   min_samples_leaf = 1,\n",
    "                   acorn = None):\n",
    "        \n",
    "       # if self.model == \"dnn\":\n",
    "       #     from honest_dnn import HonestDNN \n",
    "       # if self.model == \"uf\":\n",
    "       #     from uncertainty_forest import UncertaintyForest\n",
    "        \n",
    "        self.X_across_tasks.append(X)\n",
    "        self.y_across_tasks.append(y)\n",
    "        \n",
    "        if self.model == \"dnn\":\n",
    "            new_honest_dnn = HonestDNN(verbose = self.verbose)\n",
    "            new_honest_dnn.fit(X, y, epochs = epochs, lr = lr)\n",
    "        if self.model == \"uf\":\n",
    "            new_honest_dnn = UncertaintyForest(n_estimators = n_estimators,\n",
    "                                               max_samples = max_samples,\n",
    "                                               bootstrap = bootstrap,\n",
    "                                               max_depth = max_depth,\n",
    "                                               min_samples_leaf = min_samples_leaf,\n",
    "                                               parallel = True)\n",
    "            new_honest_dnn.fit(X, y)\n",
    "        new_transformer = new_honest_dnn.get_transformer()\n",
    "        new_voter = new_honest_dnn.get_voter()\n",
    "        new_classes = new_honest_dnn.classes_\n",
    "        \n",
    "        self.transformers_across_tasks.append(new_transformer)\n",
    "        self.classes_across_tasks.append(new_classes)\n",
    "        \n",
    "        #add one voter to previous task voter lists under the new transformation\n",
    "        for task_idx in range(self.n_tasks):\n",
    "            X_of_task, y_of_task = self.X_across_tasks[task_idx], self.y_across_tasks[task_idx]\n",
    "            if self.model == \"dnn\":\n",
    "                X_of_task_under_new_transform = new_transformer.predict(X_of_task) \n",
    "            if self.model == \"uf\":\n",
    "                X_of_task_under_new_transform = new_transformer(X_of_task) \n",
    "            unfit_task_voter_under_new_transformation = clone(self.voters_across_tasks_matrix[task_idx][0])\n",
    "            if self.model == \"uf\":\n",
    "                unfit_task_voter_under_new_transformation.classes_ = self.voters_across_tasks_matrix[task_idx][0].classes_\n",
    "            task_voter_under_new_transformation = unfit_task_voter_under_new_transformation.fit(X_of_task_under_new_transform, y_of_task)\n",
    "\n",
    "            self.voters_across_tasks_matrix[task_idx].append(task_voter_under_new_transformation)\n",
    "            \n",
    "        #add n_tasks voters to new task voter list under previous transformations \n",
    "        new_voters_under_previous_task_transformation = []\n",
    "        for task_idx in range(self.n_tasks):\n",
    "            transformer_of_task = self.transformers_across_tasks[task_idx]\n",
    "            if self.model == \"dnn\":\n",
    "                X_under_task_transformation = transformer_of_task.predict(X)\n",
    "            if self.model == \"uf\":\n",
    "                X_under_task_transformation = transformer_of_task(X)\n",
    "            unfit_new_task_voter_under_task_transformation = clone(new_voter)\n",
    "            if self.model == \"uf\":\n",
    "                unfit_new_task_voter_under_task_transformation.classes_ = new_voter.classes_\n",
    "            new_task_voter_under_task_transformation = unfit_new_task_voter_under_task_transformation.fit(X_under_task_transformation, y)\n",
    "            new_voters_under_previous_task_transformation.append(new_task_voter_under_task_transformation)\n",
    "            \n",
    "        #make sure to add the voter of the new task under its own transformation\n",
    "        new_voters_under_previous_task_transformation.append(new_voter)\n",
    "        \n",
    "        self.voters_across_tasks_matrix.append(new_voters_under_previous_task_transformation)\n",
    "        \n",
    "        self.n_tasks += 1\n",
    "        \n",
    "    def _estimate_posteriors(self, X, representation = 0, decider = 0):\n",
    "        self.check_task_idx_(decider)\n",
    "        \n",
    "        if representation == \"all\":\n",
    "            representation = range(self.n_tasks)\n",
    "        elif isinstance(representation, int):\n",
    "            representation = np.array([representation])\n",
    "        \n",
    "        def worker(transformer_task_idx):\n",
    "            transformer = self.transformers_across_tasks[transformer_task_idx]\n",
    "            voter = self.voters_across_tasks_matrix[decider][transformer_task_idx]\n",
    "            if self.model == \"dnn\":\n",
    "                return voter.predict_proba(transformer.predict(X))\n",
    "            if self.model == \"uf\":\n",
    "                return voter.predict_proba(transformer(X))\n",
    "        \n",
    "        posteriors_across_tasks = np.array(\n",
    "                    Parallel(n_jobs=-1)(\n",
    "                            delayed(worker)(transformer_task_idx) for transformer_task_idx in representation\n",
    "                    )\n",
    "            )    \n",
    "            \n",
    "        return np.mean(posteriors_across_tasks, axis = 0)\n",
    "        \n",
    "    def predict(self, X, representation = 0, decider = 0):\n",
    "        task_classes = self.classes_across_tasks[decider]\n",
    "        return task_classes[np.argmax(self._estimate_posteriors(X, representation, decider), axis = -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_data(data_x, data_y, class_idx, total_cls=100, cv=1):\n",
    "    x = data_x.copy()\n",
    "    y = data_y.copy()\n",
    "    idx = class_idx.copy()\n",
    "    \n",
    "    total_task=10\n",
    "    batch_per_task=10\n",
    "    \n",
    "    for t in range(total_task):\n",
    "        for b in range(batch_per_task):\n",
    "            for i in range(t*10,(t+1)*10,1):\n",
    "                indx = np.roll(idx[i],(cv-1)*100)\n",
    "                \n",
    "                if b==0 and i==0 and t==0:\n",
    "                    train_x = x[indx[b*50:(b+1)*50],:]\n",
    "                    train_y = y[indx[b*50:(b+1)*50]]\n",
    "                    test_x = x[indx[b*10+500:(b+1)*10+500],:]\n",
    "                    test_y = y[indx[b*10+500:(b+1)*10+500]]\n",
    "                else:\n",
    "                    train_x = np.concatenate((train_x, x[indx[b*50:(b+1)*50],:]), axis=0)\n",
    "                    train_y = np.concatenate((train_y, y[indx[b*50:(b+1)*50]]), axis=0)\n",
    "                    test_x = np.concatenate((test_x, x[indx[b*10+500:(b+1)*10+500],:]), axis=0)\n",
    "                    test_y = np.concatenate((test_y, y[indx[b*10+500:(b+1)*10+500]]), axis=0)\n",
    "                \n",
    "            \n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp(data_x, data_y, class_idx, cv, ntrees=100, acorn=None):\n",
    "    if acorn is not None:\n",
    "        np.random.seed(acorn)\n",
    "        \n",
    "    train_x, train_y, test_x, test_y = cross_val_data(data_x, data_y, class_idx, cv=cv)\n",
    "        \n",
    "    m = 1000\n",
    "    errors = [[] for i in range(100)]\n",
    "    errors_1 = np.zeros(100,dtype=float)\n",
    "    \n",
    "    lifelong_forest = LifeLongForest()\n",
    "    \n",
    "    for ii in range(100):\n",
    "        start = time.time()\n",
    "        lifelong_forest.new_forest(train_x[ii*500:(ii+1)*500,:], homogenize_labels(train_y[ii*500:(ii+1)*500]), max_depth=log2(500), n_estimators=ntrees)\n",
    "        #lifelong_forest.new_forest(train_x[(ii-1)*5000:(ii)*5000,:], homogenize_labels(train_y[(ii-1)*5000:(ii)*5000]), n_estimators=ntrees)\n",
    "        ending = time.time()\n",
    "        print(\"training time %f\"%(ending-start))\n",
    "        \n",
    "        task_id  = ii//10\n",
    "        start = time.time()\n",
    "        llf_task=lifelong_forest.predict(test_x[task_id*1000:(task_id+1)*1000,:], representation=ii, decider=ii)\n",
    "        ending = time.time()\n",
    "        print(\"single prediction time %f\"%(ending-start))\n",
    "        \n",
    "        errors_1[ii] = 1 - np.sum(llf_task == homogenize_labels(test_y[task_id*1000:(task_id+1)*1000]))/m\n",
    "        \n",
    "        for jj in range(ii+1):\n",
    "            task_id  = jj//10\n",
    "            start = time.time()\n",
    "            llf_task=lifelong_forest.predict(test_x[task_id*1000:(task_id+1)*1000,:], representation='all', decider=jj)\n",
    "            ending = time.time()\n",
    "            print(\"multitask prediction time %f\"%(ending-start))\n",
    "            \n",
    "            errors[ii].append(1 - np.sum(llf_task == homogenize_labels(test_y[task_id*1000:(task_id+1)*1000]))/m)\n",
    "            \n",
    "    with open('../result/'+'LF'+str(cv)+'.pickle', 'wb') as f:\n",
    "        pickle.dump(errors, f)\n",
    "        \n",
    "    with open('../result/'+'LF_single_task'+str(cv)+'.pickle', 'wb') as f:\n",
    "        pickle.dump(errors_1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tasks = 10\n",
    "train_file = '/data/Jayanta/continual-learning/train'\n",
    "unpickled_train = unpickle(train_file)\n",
    "train_keys = list(unpickled_train.keys())\n",
    "fine_labels = np.array(unpickled_train[train_keys[2]])\n",
    "labels = fine_labels\n",
    "\n",
    "test_file = '/data/Jayanta/continual-learning/test'\n",
    "unpickled_test = unpickle(test_file)\n",
    "test_keys = list(unpickled_test.keys())\n",
    "fine_labels = np.array(unpickled_test[test_keys[2]])\n",
    "labels_ = fine_labels\n",
    "\n",
    "data_x = np.concatenate((unpickled_train[b'data'], unpickled_test[b'data']), axis=0)\n",
    "data_y = np.concatenate((labels, labels_), axis=0)\n",
    "\n",
    "class_idx = [np.where(data_y == u)[0] for u in np.unique(data_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing 1 fold\n",
      "training time 3.870520\n",
      "single prediction time 1.831401\n",
      "multitask prediction time 1.863700\n",
      "training time 6.303535\n",
      "single prediction time 4.519262\n",
      "multitask prediction time 8.052323\n",
      "multitask prediction time 7.946631\n",
      "training time 9.995842\n",
      "single prediction time 8.603899\n",
      "multitask prediction time 23.442045\n",
      "multitask prediction time 23.226983\n",
      "multitask prediction time 22.663259\n",
      "training time 13.345757\n",
      "single prediction time 14.228296\n",
      "multitask prediction time 50.640037\n",
      "multitask prediction time 49.480968\n",
      "multitask prediction time 50.526000\n",
      "multitask prediction time 49.961329\n",
      "training time 17.279046\n",
      "single prediction time 21.767294\n",
      "multitask prediction time 97.300559\n",
      "multitask prediction time 98.165463\n",
      "multitask prediction time 98.313083\n",
      "multitask prediction time 101.243886\n",
      "multitask prediction time 96.907192\n",
      "training time 21.733353\n",
      "single prediction time 30.832241\n",
      "multitask prediction time 169.487908\n",
      "multitask prediction time 169.791888\n",
      "multitask prediction time 170.601881\n",
      "multitask prediction time 168.166303\n",
      "multitask prediction time 170.138597\n",
      "multitask prediction time 167.765216\n",
      "training time 27.203735\n",
      "single prediction time 41.883102\n",
      "multitask prediction time 261.070091\n",
      "multitask prediction time 265.965979\n",
      "multitask prediction time 267.122104\n",
      "multitask prediction time 269.475558\n",
      "multitask prediction time 264.904665\n",
      "multitask prediction time 268.452599\n",
      "multitask prediction time 264.320573\n",
      "training time 29.869890\n",
      "single prediction time 52.552123\n",
      "multitask prediction time 390.807689\n",
      "multitask prediction time 395.300498\n",
      "multitask prediction time 398.997576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/jayanta/.local/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py\", line 558, in run\n",
      "    self.terminate_broken(bpe)\n",
      "  File \"/home/jayanta/.local/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py\", line 736, in terminate_broken\n",
      "    self.kill_workers()\n",
      "  File \"/home/jayanta/.local/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py\", line 766, in kill_workers\n",
      "    recursive_terminate(p)\n",
      "  File \"/home/jayanta/.local/lib/python3.6/site-packages/joblib/externals/loky/backend/utils.py\", line 28, in recursive_terminate\n",
      "    _recursive_terminate_without_psutil(process)\n",
      "  File \"/home/jayanta/.local/lib/python3.6/site-packages/joblib/externals/loky/backend/utils.py\", line 53, in _recursive_terminate_without_psutil\n",
      "    _recursive_terminate(process.pid)\n",
      "  File \"/home/jayanta/.local/lib/python3.6/site-packages/joblib/externals/loky/backend/utils.py\", line 94, in _recursive_terminate\n",
      "    stderr=None\n",
      "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
      "    **kwargs).stdout\n",
      "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
      "    output=stdout, stderr=stderr)\n",
      "subprocess.CalledProcessError: Command '['pgrep', '-P', '51378']' died with <Signals.SIGINT: 2>.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-3393b90c1846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Doing %d fold\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntrees\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macorn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-0aef8760ab21>\u001b[0m in \u001b[0;36mexp\u001b[0;34m(data_x, data_y, class_idx, cv, ntrees, acorn)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mtask_id\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mjj\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mllf_task\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlifelong_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepresentation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecider\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multitask prediction time %f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mending\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-731152538b3e>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, representation, decider)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepresentation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecider\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mtask_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_across_tasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecider\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtask_classes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimate_posteriors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepresentation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecider\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-731152538b3e>\u001b[0m in \u001b[0;36m_estimate_posteriors\u001b[0;34m(self, X, representation, decider)\u001b[0m\n\u001b[1;32m    114\u001b[0m         posteriors_across_tasks = np.array(\n\u001b[1;32m    115\u001b[0m                     Parallel(n_jobs=-1)(\n\u001b[0;32m--> 116\u001b[0;31m                             \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer_task_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtransformer_task_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrepresentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                     )\n\u001b[1;32m    118\u001b[0m             )    \n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    941\u001b[0m                     \u001b[0;31m# scheduling.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mabort_everything\u001b[0;34m(self, ensure_ready)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \"\"\"Shutdown the workers and restart a new one with the same parameters\n\u001b[1;32m    558\u001b[0m         \"\"\"\n\u001b[0;32m--> 559\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/joblib/executor.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self, kill_workers)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkill_workers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;31m# When workers are killed in such a brutal manner, they cannot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, kill_workers)\u001b[0m\n\u001b[1;32m   1169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexecutor_manager_thread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m             \u001b[0mexecutor_manager_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         \u001b[0;31m# To reduce the risk of opening too many files, remove references to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Parallel(n_jobs=-2,verbose=1)(\n",
    "#    delayed(exp)(\n",
    "#        data_x, data_y, class_idx, i, ntrees=100, acorn=i) for i in range(1,7,1)\n",
    "#)\n",
    "\n",
    "for i in range(1,7,1):\n",
    "    print(\"Doing %d fold\"%i)\n",
    "    exp(data_x, data_y, class_idx, i, ntrees=200, acorn=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.341934204101562e-05"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "end = time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
