{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from itertools import product\n",
    "from math import log2\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "#Infrastructure\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import NotFittedError\n",
    "\n",
    "#Data Handling\n",
    "from sklearn.utils.validation import (\n",
    "    check_X_y,\n",
    "    check_array,\n",
    "    NotFittedError,\n",
    ")\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "\n",
    "#Utils\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import clone \n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "\n",
    "#%%\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _finite_sample_correction(posteriors, num_points_in_partition, num_classes):\n",
    "    '''\n",
    "    encourage posteriors to approach uniform when there is low data\n",
    "    '''\n",
    "    correction_constant = 1 / (num_classes * num_points_in_partition)\n",
    "\n",
    "    zero_posterior_idxs = np.where(posteriors == 0)[0]\n",
    "    posteriors[zero_posterior_idxs] = correction_constant\n",
    "    \n",
    "    posteriors /= sum(posteriors)\n",
    "    \n",
    "    return posteriors\n",
    "\n",
    "class UncertaintyForest(BaseEstimator, ClassifierMixin):\n",
    "    '''\n",
    "    based off of https://arxiv.org/pdf/1907.00325.pdf\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_depth=30,\n",
    "        min_samples_leaf=1,\n",
    "        max_samples = 0.63,\n",
    "        max_features_tree = \"auto\",\n",
    "        n_estimators=200,\n",
    "        bootstrap=False,\n",
    "        parallel=True):\n",
    "\n",
    "        #Tree parameters.\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features_tree = max_features_tree\n",
    "\n",
    "        #Bag parameters\n",
    "        self.n_estimators = n_estimators\n",
    "        self.bootstrap = bootstrap\n",
    "        self.max_samples = max_samples\n",
    "\n",
    "        #Model parameters.\n",
    "        self.parallel = parallel\n",
    "        self.fitted = False\n",
    "\n",
    "    def _check_fit(self):\n",
    "        '''\n",
    "        raise a NotFittedError if the model isn't fit\n",
    "        '''\n",
    "        if not self.fitted:\n",
    "                msg = (\n",
    "                        \"This %(name)s instance is not fitted yet. Call 'fit' with \"\n",
    "                        \"appropriate arguments before using this estimator.\"\n",
    "                )\n",
    "                raise NotFittedError(msg % {\"name\": type(self).__name__})\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        get the estimated posteriors across trees\n",
    "        '''\n",
    "        X = check_array(X)\n",
    "                \n",
    "        def worker(tree_idx, tree):\n",
    "            #get the nodes of X\n",
    "            # Drop each estimation example down the tree, and record its 'y' value.\n",
    "            return tree.apply(X)\n",
    "            \n",
    "\n",
    "        if self.parallel:\n",
    "            return np.array(\n",
    "                    Parallel(n_jobs=-1)(\n",
    "                            delayed(worker)(tree_idx, tree) for tree_idx, tree in enumerate(self.ensemble.estimators_)\n",
    "                    )\n",
    "            )         \n",
    "        else:\n",
    "            return np.array(\n",
    "                    [worker(tree_idx, tree) for tree_idx, tree in enumerate(self.ensemble.estimators_)]\n",
    "                    )\n",
    "        \n",
    "    def get_transformer(self):\n",
    "        return lambda X : self.transform(X)\n",
    "        \n",
    "    def vote(self, nodes_across_trees):\n",
    "        return self.voter.predict(nodes_across_trees)\n",
    "        \n",
    "    def get_voter(self):\n",
    "        return self.voter\n",
    "        \n",
    "                        \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        #format X and y\n",
    "        X, y = check_X_y(X, y)\n",
    "        check_classification_targets(y)\n",
    "        self.classes_, y = np.unique(y, return_inverse=True)\n",
    "        \n",
    "        #define the ensemble\n",
    "        self.ensemble = BaggingClassifier(\n",
    "            DecisionTreeClassifier(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                max_features=self.max_features_tree\n",
    "            ),\n",
    "            n_estimators=self.n_estimators,\n",
    "            max_samples=self.max_samples,\n",
    "            bootstrap=self.bootstrap,\n",
    "            n_jobs = -1\n",
    "        )\n",
    "        \n",
    "        #fit the ensemble\n",
    "        self.ensemble.fit(X, y)\n",
    "        \n",
    "        class Voter(BaseEstimator):\n",
    "            def __init__(self, estimators_samples_, classes, parallel = True):\n",
    "                self.n_estimators = len(estimators_samples_)\n",
    "                self.classes_ = classes\n",
    "                self.parallel = parallel\n",
    "                self.estimators_samples_ = estimators_samples_\n",
    "            \n",
    "            def fit(self, nodes_across_trees, y, fitting = False):\n",
    "                self.tree_idx_to_node_ids_to_posterior_map = {}\n",
    "\n",
    "                def worker(tree_idx):\n",
    "                    nodes = nodes_across_trees[tree_idx]\n",
    "                    oob_samples = np.delete(range(len(nodes)), self.estimators_samples_[tree_idx])\n",
    "                    cal_nodes = nodes[oob_samples] if fitting else nodes\n",
    "                    y_cal = y[oob_samples] if fitting else y                    \n",
    "                    \n",
    "                    #create a map from the unique node ids to their classwise posteriors\n",
    "                    node_ids_to_posterior_map = {}\n",
    "\n",
    "                    #fill in the posteriors \n",
    "                    for node_id in np.unique(cal_nodes):\n",
    "                        cal_idxs_of_node_id = np.where(cal_nodes == node_id)[0]\n",
    "                        cal_ys_of_node = y_cal[cal_idxs_of_node_id]\n",
    "                        class_counts = [len(np.where(cal_ys_of_node == y)[0]) for y in np.unique(y) ]\n",
    "                        posteriors = np.nan_to_num(np.array(class_counts) / np.sum(class_counts))\n",
    "\n",
    "                        #finite sample correction\n",
    "                        posteriors_corrected = _finite_sample_correction(posteriors, len(cal_idxs_of_node_id), len(self.classes_))\n",
    "                        node_ids_to_posterior_map[node_id] = posteriors_corrected\n",
    "                        \n",
    "                    #add the node_ids_to_posterior_map to the overall tree_idx map \n",
    "                    self.tree_idx_to_node_ids_to_posterior_map[tree_idx] = node_ids_to_posterior_map\n",
    "                    \n",
    "                for tree_idx in range(self.n_estimators):\n",
    "                        worker(tree_idx)\n",
    "                return self\n",
    "                        \n",
    "                        \n",
    "            def predict_proba(self, nodes_across_trees):\n",
    "                def worker(tree_idx):\n",
    "                    #get the node_ids_to_posterior_map for this tree\n",
    "                    node_ids_to_posterior_map = self.tree_idx_to_node_ids_to_posterior_map[tree_idx]\n",
    "\n",
    "                    #get the nodes of X\n",
    "                    nodes = nodes_across_trees[tree_idx]\n",
    "\n",
    "                    posteriors = []\n",
    "                    node_ids = node_ids_to_posterior_map.keys()\n",
    "\n",
    "                    #loop over nodes of X\n",
    "                    for node in nodes:\n",
    "                        #if we've seen this node before, simply get the posterior\n",
    "                        if node in node_ids:\n",
    "                            posteriors.append(node_ids_to_posterior_map[node])\n",
    "                        #if we haven't seen this node before, simply use the uniform posterior \n",
    "                        else:\n",
    "                            posteriors.append(np.ones((len(np.unique(self.classes_)))) / len(self.classes_))\n",
    "                    return posteriors\n",
    "\n",
    "                if self.parallel:\n",
    "                    return np.mean(\n",
    "                            Parallel(n_jobs=-1)(\n",
    "                                    delayed(worker)(tree_idx) for tree_idx in range(self.n_estimators)\n",
    "                            ), axis = 0\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    return np.mean(\n",
    "                            [worker(tree_idx) for tree_idx in range(self.n_estimators)], axis = 0)\n",
    "                \n",
    "        #get the nodes of the calibration set\n",
    "        nodes_across_trees = self.transform(X) \n",
    "        self.voter = Voter(estimators_samples_ = self.ensemble.estimators_samples_, classes = self.classes_, parallel = self.parallel)\n",
    "        self.voter.fit(nodes_across_trees, y, fitting = True)\n",
    "        self.fitted = True\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.classes_[np.argmax(self.predict_proba(X), axis=-1)]\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.voter.predict_proba(self.transform(X))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LifeLongForest():\n",
    "    def __init__(self, acorn = None, verbose = False, model = \"uf\"):\n",
    "        self.X_across_tasks = []\n",
    "        self.y_across_tasks = []\n",
    "        \n",
    "        self.transformers_across_tasks = []\n",
    "        \n",
    "        #element [i, j] votes on decider from task i under representation from task j\n",
    "        self.voters_across_tasks_matrix = []\n",
    "        self.n_tasks = 0\n",
    "        \n",
    "        self.classes_across_tasks = []\n",
    "        \n",
    "        if acorn is not None:\n",
    "            np.random.seed(acorn)\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "    def check_task_idx_(self, task_idx):\n",
    "        if task_idx >= self.n_tasks:\n",
    "            raise Exception(\"Invalid Task IDX\")\n",
    "    \n",
    "    def new_forest(self, \n",
    "                   X, \n",
    "                   y, \n",
    "                   epochs = 100, \n",
    "                   lr = 5e-4, \n",
    "                   n_estimators = 10, \n",
    "                   max_samples = .63,\n",
    "                   bootstrap = False,\n",
    "                   max_depth = 30,\n",
    "                   min_samples_leaf = 1,\n",
    "                   acorn = None):\n",
    "        \n",
    "        #if self.model == \"dnn\":\n",
    "        #    from honest_dnn import HonestDNN \n",
    "        #if self.model == \"uf\":\n",
    "        #    from uncertainty_forest import UncertaintyForest\n",
    "        \n",
    "        self.X_across_tasks.append(X)\n",
    "        self.y_across_tasks.append(y)\n",
    "        \n",
    "        if self.model == \"dnn\":\n",
    "            new_honest_dnn = HonestDNN(verbose = self.verbose)\n",
    "            new_honest_dnn.fit(X, y, epochs = epochs, lr = lr)\n",
    "        if self.model == \"uf\":\n",
    "            new_honest_dnn = UncertaintyForest(n_estimators = n_estimators,\n",
    "                                               max_samples = max_samples,\n",
    "                                               bootstrap = bootstrap,\n",
    "                                               max_depth = max_depth,\n",
    "                                               min_samples_leaf = min_samples_leaf,\n",
    "                                               parallel = True)\n",
    "            new_honest_dnn.fit(X, y)\n",
    "        new_transformer = new_honest_dnn.get_transformer()\n",
    "        new_voter = new_honest_dnn.get_voter()\n",
    "        new_classes = new_honest_dnn.classes_\n",
    "        \n",
    "        self.transformers_across_tasks.append(new_transformer)\n",
    "        self.classes_across_tasks.append(new_classes)\n",
    "        \n",
    "        #add one voter to previous task voter lists under the new transformation\n",
    "        for task_idx in range(self.n_tasks):\n",
    "            X_of_task, y_of_task = self.X_across_tasks[task_idx], self.y_across_tasks[task_idx]\n",
    "            if self.model == \"dnn\":\n",
    "                X_of_task_under_new_transform = new_transformer.predict(X_of_task) \n",
    "            if self.model == \"uf\":\n",
    "                X_of_task_under_new_transform = new_transformer(X_of_task) \n",
    "            unfit_task_voter_under_new_transformation = clone(self.voters_across_tasks_matrix[task_idx][0])\n",
    "            if self.model == \"uf\":\n",
    "                unfit_task_voter_under_new_transformation.classes_ = self.voters_across_tasks_matrix[task_idx][0].classes_\n",
    "            task_voter_under_new_transformation = unfit_task_voter_under_new_transformation.fit(X_of_task_under_new_transform, y_of_task)\n",
    "\n",
    "            self.voters_across_tasks_matrix[task_idx].append(task_voter_under_new_transformation)\n",
    "            \n",
    "        #add n_tasks voters to new task voter list under previous transformations \n",
    "        new_voters_under_previous_task_transformation = []\n",
    "        for task_idx in range(self.n_tasks):\n",
    "            transformer_of_task = self.transformers_across_tasks[task_idx]\n",
    "            if self.model == \"dnn\":\n",
    "                X_under_task_transformation = transformer_of_task.predict(X)\n",
    "            if self.model == \"uf\":\n",
    "                X_under_task_transformation = transformer_of_task(X)\n",
    "            unfit_new_task_voter_under_task_transformation = clone(new_voter)\n",
    "            if self.model == \"uf\":\n",
    "                unfit_new_task_voter_under_task_transformation.classes_ = new_voter.classes_\n",
    "            new_task_voter_under_task_transformation = unfit_new_task_voter_under_task_transformation.fit(X_under_task_transformation, y)\n",
    "            new_voters_under_previous_task_transformation.append(new_task_voter_under_task_transformation)\n",
    "            \n",
    "        #make sure to add the voter of the new task under its own transformation\n",
    "        new_voters_under_previous_task_transformation.append(new_voter)\n",
    "        \n",
    "        self.voters_across_tasks_matrix.append(new_voters_under_previous_task_transformation)\n",
    "        \n",
    "        self.n_tasks += 1\n",
    "        \n",
    "    def _estimate_posteriors(self, X, representation = 0, decider = 0):\n",
    "        self.check_task_idx_(decider)\n",
    "        \n",
    "        if representation == \"all\":\n",
    "            representation = range(self.n_tasks)\n",
    "        elif isinstance(representation, int):\n",
    "            representation = np.array([representation])\n",
    "        \n",
    "        posteriors_across_tasks = []\n",
    "        for transformer_task_idx in representation:\n",
    "            transformer = self.transformers_across_tasks[transformer_task_idx]\n",
    "            voter = self.voters_across_tasks_matrix[decider][transformer_task_idx]\n",
    "            if self.model == \"dnn\":\n",
    "                posteriors_across_tasks.append(voter.predict_proba(transformer.predict(X)))\n",
    "            if self.model == \"uf\":\n",
    "                posteriors_across_tasks.append(voter.predict_proba(transformer(X)))\n",
    "        return np.mean(posteriors_across_tasks, axis = 0)\n",
    "    \n",
    "    def predict(self, X, representation = 0, decider = 0):\n",
    "        task_classes = self.classes_across_tasks[decider]\n",
    "        return task_classes[np.argmax(self._estimate_posteriors(X, representation, decider), axis = -1)] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_data(data_x, data_y, class_idx, task, total_cls=100, cv=1):\n",
    "    x = data_x.copy()\n",
    "    y = data_y.copy()\n",
    "    idx = class_idx.copy()\n",
    "    \n",
    "    #total_task=10\n",
    "    batch_per_task=10\n",
    "    \n",
    "    for t in [task]:\n",
    "        for b in range(batch_per_task):\n",
    "            for i in range(t*10,(t+1)*10,1):\n",
    "                indx = np.roll(idx[i],(cv-1)*100)\n",
    "                \n",
    "                if b==0 and i==t*10:\n",
    "                    train_x = x[indx[b*50:(b+1)*50],:]\n",
    "                    train_y = y[indx[b*50:(b+1)*50]]\n",
    "                    test_x = x[indx[b*10+500:(b+1)*10+500],:]\n",
    "                    test_y = y[indx[b*10+500:(b+1)*10+500]]\n",
    "                else:\n",
    "                    train_x = np.concatenate((train_x, x[indx[b*50:(b+1)*50],:]), axis=0)\n",
    "                    train_y = np.concatenate((train_y, y[indx[b*50:(b+1)*50]]), axis=0)\n",
    "                    test_x = np.concatenate((test_x, x[indx[b*10+500:(b+1)*10+500],:]), axis=0)\n",
    "                    test_y = np.concatenate((test_y, y[indx[b*10+500:(b+1)*10+500]]), axis=0)\n",
    "                \n",
    "            \n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LF_experiment(train_x, train_y, test_x, test_y, task, ntrees, cv, acorn=None):\n",
    "    if acorn is not None:\n",
    "        np.random.seed(acorn)\n",
    "       \n",
    "    m=1000\n",
    "    err=0\n",
    "    \n",
    "    for i in range(10):\n",
    "        lifelong_forest = LifeLongForest()\n",
    "        lifelong_forest.new_forest(train_x[i*500:(i+1)*500,:], \n",
    "                               train_y[i*500:(i+1)*500], max_depth=log2(500), n_estimators=ntrees)\n",
    "        \n",
    "     \n",
    "        llf_single_task=lifelong_forest.predict(test_x, representation=0, decider=0)\n",
    "        err += 1 - np.sum(llf_single_task == test_y)/m\n",
    "    \n",
    "    return err/10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel_exp(data_x, data_y, class_idx, ntrees, task):\n",
    "    \n",
    "    err = 0\n",
    "    for i in range(1,7):\n",
    "        train_x, train_y, test_x, test_y = cross_val_data(data_x, data_y, class_idx, task, cv=i)\n",
    "        err += LF_experiment(train_x, train_y, test_x, test_y, task, ntrees, i, acorn=task)\n",
    "        \n",
    "    err /= 6\n",
    "    \n",
    "    with open('../result/task_minibatch'+str(task)+'__'+str(ntrees),'wb') as f:\n",
    "        pickle.dump(err,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tasks = 10\n",
    "train_file = '/data/Jayanta/continual-learning/train'\n",
    "unpickled_train = unpickle(train_file)\n",
    "train_keys = list(unpickled_train.keys())\n",
    "fine_labels = np.array(unpickled_train[train_keys[2]])\n",
    "labels = fine_labels\n",
    "\n",
    "\n",
    "test_file = '/data/Jayanta/continual-learning/test'\n",
    "unpickled_test = unpickle(test_file)\n",
    "test_keys = list(unpickled_test.keys())\n",
    "fine_labels = np.array(unpickled_test[test_keys[2]])\n",
    "labels_ = fine_labels\n",
    "\n",
    "data_x = np.concatenate((unpickled_train[b'data'], unpickled_test[b'data']), axis=0)\n",
    "data_y = np.concatenate((labels, labels_), axis=0)\n",
    "\n",
    "class_idx = [np.where(data_y == u)[0] for u in np.unique(data_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done 140 tasks      | elapsed: 32.9min\n",
      "[Parallel(n_jobs=30)]: Done 290 out of 290 | elapsed: 66.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trees = range(10,300,10)\n",
    "task = range(0,10)\n",
    "iterable = product(task,trees)\n",
    "\n",
    "Parallel(n_jobs=30,verbose=1)(delayed(run_parallel_exp)(data_x, data_y, class_idx, ntrees, task_) for task_,ntrees in iterable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
