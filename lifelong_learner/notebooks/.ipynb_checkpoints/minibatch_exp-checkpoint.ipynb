{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "import pickle\n",
    "from itertools import product\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "#Infrastructure\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import NotFittedError\n",
    "\n",
    "#Data Handling\n",
    "from sklearn.utils.validation import (\n",
    "    check_X_y,\n",
    "    check_array,\n",
    "    NotFittedError,\n",
    ")\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "\n",
    "#Utils\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.base import clone \n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def homogenize_labels(a):\n",
    "    u = np.unique(a)\n",
    "    return np.array([np.where(u == i)[0][0] for i in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _finite_sample_correction(posteriors, num_points_in_partition, num_classes):\n",
    "    '''\n",
    "    encourage posteriors to approach uniform when there is low data\n",
    "    '''\n",
    "    correction_constant = 1 / (num_classes * num_points_in_partition)\n",
    "\n",
    "    zero_posterior_idxs = np.where(posteriors == 0)[0]\n",
    "\n",
    "    c = len(zero_posterior_idxs) / (num_classes * num_points_in_partition)\n",
    "    posteriors *= (1 - c)\n",
    "    posteriors[zero_posterior_idxs] = correction_constant\n",
    "    return posteriors\n",
    "\n",
    "class UncertaintyForest(BaseEstimator, ClassifierMixin):\n",
    "    '''\n",
    "    based off of https://arxiv.org/pdf/1907.00325.pdf\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_depth=30,\n",
    "        min_samples_leaf=1,\n",
    "        max_samples = 0.32,\n",
    "        max_features_tree = \"auto\",\n",
    "        n_estimators=200,\n",
    "        bootstrap=False,\n",
    "        parallel=True):\n",
    "\n",
    "        #Tree parameters.\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features_tree = max_features_tree\n",
    "\n",
    "        #Bag parameters\n",
    "        self.n_estimators = n_estimators\n",
    "        self.bootstrap = bootstrap\n",
    "        self.max_samples = max_samples\n",
    "\n",
    "        #Model parameters.\n",
    "        self.parallel = parallel\n",
    "        self.fitted = False\n",
    "\n",
    "    def _check_fit(self):\n",
    "        '''\n",
    "        raise a NotFittedError if the model isn't fit\n",
    "        '''\n",
    "        if not self.fitted:\n",
    "                msg = (\n",
    "                        \"This %(name)s instance is not fitted yet. Call 'fit' with \"\n",
    "                        \"appropriate arguments before using this estimator.\"\n",
    "                )\n",
    "                raise NotFittedError(msg % {\"name\": type(self).__name__})\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        get the estimated posteriors across trees\n",
    "        '''\n",
    "        X = check_array(X)\n",
    "                \n",
    "        def worker(tree_idx, tree):\n",
    "            #get the nodes of X\n",
    "            # Drop each estimation example down the tree, and record its 'y' value.\n",
    "            return tree.apply(X)\n",
    "            \n",
    "\n",
    "        if self.parallel:\n",
    "            return np.array(\n",
    "                    Parallel(n_jobs=-1)(\n",
    "                            delayed(worker)(tree_idx, tree) for tree_idx, tree in enumerate(self.ensemble.estimators_)\n",
    "                    )\n",
    "            )         \n",
    "        else:\n",
    "            return np.array(\n",
    "                    [worker(tree_idx, tree) for tree_idx, tree in enumerate(self.ensemble.estimators_)]\n",
    "                    )\n",
    "        \n",
    "    def get_transformer(self):\n",
    "        return lambda X : self.transform(X)\n",
    "        \n",
    "    def vote(self, nodes_across_trees):\n",
    "        return self.voter.predict(nodes_across_trees)\n",
    "        \n",
    "    def get_voter(self):\n",
    "        return self.voter\n",
    "        \n",
    "                        \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        #format X and y\n",
    "        X, y = check_X_y(X, y)\n",
    "        check_classification_targets(y)\n",
    "        self.classes_, y = np.unique(y, return_inverse=True)\n",
    "        \n",
    "        #define the ensemble\n",
    "        self.ensemble = BaggingClassifier(\n",
    "            DecisionTreeClassifier(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                max_features=self.max_features_tree\n",
    "            ),\n",
    "            n_estimators=self.n_estimators,\n",
    "            max_samples=self.max_samples,\n",
    "            bootstrap=self.bootstrap,\n",
    "            n_jobs = -1\n",
    "        )\n",
    "        \n",
    "        #fit the ensemble\n",
    "        self.ensemble.fit(X, y)\n",
    "        \n",
    "        class Voter(BaseEstimator):\n",
    "            def __init__(self, estimators_samples_, classes, parallel = True):\n",
    "                self.n_estimators = len(estimators_samples_)\n",
    "                self.classes_ = classes\n",
    "                self.parallel = parallel\n",
    "                self.estimators_samples_ = estimators_samples_\n",
    "            \n",
    "            def fit(self, nodes_across_trees, y, fitting = False):\n",
    "                self.tree_idx_to_node_ids_to_posterior_map = {}\n",
    "\n",
    "                def worker(tree_idx):\n",
    "                    nodes = nodes_across_trees[tree_idx]\n",
    "                    cal_nodes = nodes[self.estimators_samples_[tree_idx]] if fitting else nodes\n",
    "                    y_cal = y[self.estimators_samples_[tree_idx]] if fitting else y                    \n",
    "                    \n",
    "                    #create a map from the unique node ids to their classwise posteriors\n",
    "                    node_ids_to_posterior_map = {}\n",
    "\n",
    "                    #fill in the posteriors \n",
    "                    for node_id in np.unique(cal_nodes):\n",
    "                        cal_idxs_of_node_id = np.where(cal_nodes == node_id)[0]\n",
    "                        cal_ys_of_node = y_cal[cal_idxs_of_node_id]\n",
    "                        class_counts = [len(np.where(cal_ys_of_node == y)[0]) for y in np.unique(y) ]\n",
    "                        posteriors = np.nan_to_num(np.array(class_counts) / np.sum(class_counts))\n",
    "\n",
    "                        #finite sample correction\n",
    "                        posteriors_corrected = _finite_sample_correction(posteriors, len(cal_idxs_of_node_id), len(self.classes_))\n",
    "                        node_ids_to_posterior_map[node_id] = posteriors_corrected\n",
    "\n",
    "                    #add the node_ids_to_posterior_map to the overall tree_idx map \n",
    "                    self.tree_idx_to_node_ids_to_posterior_map[tree_idx] = node_ids_to_posterior_map\n",
    "                    \n",
    "                for tree_idx in range(self.n_estimators):\n",
    "                        worker(tree_idx)\n",
    "                return self\n",
    "                        \n",
    "                        \n",
    "            def predict_proba(self, nodes_across_trees):\n",
    "                def worker(tree_idx):\n",
    "                    #get the node_ids_to_posterior_map for this tree\n",
    "                    node_ids_to_posterior_map = self.tree_idx_to_node_ids_to_posterior_map[tree_idx]\n",
    "\n",
    "                    #get the nodes of X\n",
    "                    nodes = nodes_across_trees[tree_idx]\n",
    "\n",
    "                    posteriors = []\n",
    "                    node_ids = node_ids_to_posterior_map.keys()\n",
    "\n",
    "                    #loop over nodes of X\n",
    "                    for node in nodes:\n",
    "                        #if we've seen this node before, simply get the posterior\n",
    "                        if node in node_ids:\n",
    "                            posteriors.append(node_ids_to_posterior_map[node])\n",
    "                        #if we haven't seen this node before, simply use the uniform posterior \n",
    "                        else:\n",
    "                            posteriors.append(np.ones((len(np.unique(self.classes_)))) / len(self.classes_))\n",
    "                    return posteriors\n",
    "\n",
    "                if self.parallel:\n",
    "                    return np.mean(\n",
    "                            Parallel(n_jobs=-1)(\n",
    "                                    delayed(worker)(tree_idx) for tree_idx in range(self.n_estimators)\n",
    "                            ), axis = 0\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    return np.mean(\n",
    "                            [worker(tree_idx) for tree_idx in range(self.n_estimators)], axis = 0)\n",
    "                \n",
    "        #get the nodes of the calibration set\n",
    "        nodes_across_trees = self.transform(X) \n",
    "        self.voter = Voter(estimators_samples_ = self.ensemble.estimators_samples_, classes = self.classes_, parallel = self.parallel)\n",
    "        self.voter.fit(nodes_across_trees, y, fitting = True)\n",
    "        self.fitted = True\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.classes_[np.argmax(self.predict_proba(X), axis=-1)]\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.voter.predict_proba(self.transform(X))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LifeLongForest():\n",
    "    def __init__(self, acorn = None, verbose = False, model = \"uf\"):\n",
    "        self.X_across_tasks = []\n",
    "        self.y_across_tasks = []\n",
    "        \n",
    "        self.transformers_across_tasks = []\n",
    "        \n",
    "        #element [i, j] votes on decider from task i under representation from task j\n",
    "        self.voters_across_tasks_matrix = []\n",
    "        self.n_tasks = 0\n",
    "        \n",
    "        self.classes_across_tasks = []\n",
    "        \n",
    "        if acorn is not None:\n",
    "            np.random.seed(acorn)\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "    def check_task_idx_(self, task_idx):\n",
    "        if task_idx >= self.n_tasks:\n",
    "            raise Exception(\"Invalid Task IDX\")\n",
    "    \n",
    "    def new_forest(self, \n",
    "                   X, \n",
    "                   y, \n",
    "                   epochs = 100, \n",
    "                   lr = 5e-4, \n",
    "                   n_estimators = 100, \n",
    "                   max_samples = .63,\n",
    "                   bootstrap = False,\n",
    "                   max_depth = 30,\n",
    "                   min_samples_leaf = 1,\n",
    "                   acorn = None):\n",
    "        \n",
    "       # if self.model == \"dnn\":\n",
    "       #     from honest_dnn import HonestDNN \n",
    "       # if self.model == \"uf\":\n",
    "       #     from uncertainty_forest import UncertaintyForest\n",
    "        \n",
    "        self.X_across_tasks.append(X)\n",
    "        self.y_across_tasks.append(y)\n",
    "        \n",
    "        if self.model == \"dnn\":\n",
    "            new_honest_dnn = HonestDNN(verbose = self.verbose)\n",
    "            new_honest_dnn.fit(X, y, epochs = epochs, lr = lr)\n",
    "        if self.model == \"uf\":\n",
    "            new_honest_dnn = UncertaintyForest(n_estimators = n_estimators,\n",
    "                                               max_samples = max_samples,\n",
    "                                               bootstrap = bootstrap,\n",
    "                                               max_depth = max_depth,\n",
    "                                               min_samples_leaf = min_samples_leaf,\n",
    "                                               parallel = True)\n",
    "            new_honest_dnn.fit(X, y)\n",
    "        new_transformer = new_honest_dnn.get_transformer()\n",
    "        new_voter = new_honest_dnn.get_voter()\n",
    "        new_classes = new_honest_dnn.classes_\n",
    "        \n",
    "        self.transformers_across_tasks.append(new_transformer)\n",
    "        self.classes_across_tasks.append(new_classes)\n",
    "        \n",
    "        #add one voter to previous task voter lists under the new transformation\n",
    "        for task_idx in range(self.n_tasks):\n",
    "            X_of_task, y_of_task = self.X_across_tasks[task_idx], self.y_across_tasks[task_idx]\n",
    "            if self.model == \"dnn\":\n",
    "                X_of_task_under_new_transform = new_transformer.predict(X_of_task) \n",
    "            if self.model == \"uf\":\n",
    "                X_of_task_under_new_transform = new_transformer(X_of_task) \n",
    "            unfit_task_voter_under_new_transformation = clone(self.voters_across_tasks_matrix[task_idx][0])\n",
    "            if self.model == \"uf\":\n",
    "                unfit_task_voter_under_new_transformation.classes_ = self.voters_across_tasks_matrix[task_idx][0].classes_\n",
    "            task_voter_under_new_transformation = unfit_task_voter_under_new_transformation.fit(X_of_task_under_new_transform, y_of_task)\n",
    "\n",
    "            self.voters_across_tasks_matrix[task_idx].append(task_voter_under_new_transformation)\n",
    "            \n",
    "        #add n_tasks voters to new task voter list under previous transformations \n",
    "        new_voters_under_previous_task_transformation = []\n",
    "        for task_idx in range(self.n_tasks):\n",
    "            transformer_of_task = self.transformers_across_tasks[task_idx]\n",
    "            if self.model == \"dnn\":\n",
    "                X_under_task_transformation = transformer_of_task.predict(X)\n",
    "            if self.model == \"uf\":\n",
    "                X_under_task_transformation = transformer_of_task(X)\n",
    "            unfit_new_task_voter_under_task_transformation = clone(new_voter)\n",
    "            if self.model == \"uf\":\n",
    "                unfit_new_task_voter_under_task_transformation.classes_ = new_voter.classes_\n",
    "            new_task_voter_under_task_transformation = unfit_new_task_voter_under_task_transformation.fit(X_under_task_transformation, y)\n",
    "            new_voters_under_previous_task_transformation.append(new_task_voter_under_task_transformation)\n",
    "            \n",
    "        #make sure to add the voter of the new task under its own transformation\n",
    "        new_voters_under_previous_task_transformation.append(new_voter)\n",
    "        \n",
    "        self.voters_across_tasks_matrix.append(new_voters_under_previous_task_transformation)\n",
    "        \n",
    "        self.n_tasks += 1\n",
    "        \n",
    "    def _estimate_posteriors(self, X, representation = 0, decider = 0):\n",
    "        self.check_task_idx_(decider)\n",
    "        \n",
    "        if representation == \"all\":\n",
    "            representation = range(self.n_tasks)\n",
    "        elif isinstance(representation, int):\n",
    "            representation = np.array([representation])\n",
    "        \n",
    "        def worker(transformer_task_idx):\n",
    "            transformer = self.transformers_across_tasks[transformer_task_idx]\n",
    "            voter = self.voters_across_tasks_matrix[decider][transformer_task_idx]\n",
    "            if self.model == \"dnn\":\n",
    "                return voter.predict_proba(transformer.predict(X))\n",
    "            if self.model == \"uf\":\n",
    "                return voter.predict_proba(transformer(X))\n",
    "        \n",
    "        posteriors_across_tasks = np.array(\n",
    "                    Parallel(n_jobs=-1)(\n",
    "                            delayed(worker)(transformer_task_idx) for transformer_task_idx in representation\n",
    "                    )\n",
    "            )    \n",
    "            \n",
    "        return np.mean(posteriors_across_tasks, axis = 0)\n",
    "        \n",
    "    def predict(self, X, representation = 0, decider = 0):\n",
    "        task_classes = self.classes_across_tasks[decider]\n",
    "        return task_classes[np.argmax(self._estimate_posteriors(X, representation, decider), axis = -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_data(data_x, data_y, class_idx, total_cls=100, cv=1):\n",
    "    x = data_x.copy()\n",
    "    y = data_y.copy()\n",
    "    idx = class_idx.copy()\n",
    "    \n",
    "    total_task=10\n",
    "    batch_per_task=10\n",
    "    \n",
    "    for t in range(total_task):\n",
    "        for b in range(batch_per_task):\n",
    "            for i in range(t*10,(t+1)*10,1):\n",
    "                indx = np.roll(idx[i],(cv-1)*100)\n",
    "                \n",
    "                if b==0 and i==0 and t==0:\n",
    "                    train_x = x[indx[b*50:(b+1)*50],:]\n",
    "                    train_y = y[indx[b*50:(b+1)*50]]\n",
    "                    test_x = x[indx[b*10+500:(b+1)*10+500],:]\n",
    "                    test_y = y[indx[b*10+500:(b+1)*10+500]]\n",
    "                else:\n",
    "                    train_x = np.concatenate((train_x, x[indx[b*50:(b+1)*50],:]), axis=0)\n",
    "                    train_y = np.concatenate((train_y, y[indx[b*50:(b+1)*50]]), axis=0)\n",
    "                    test_x = np.concatenate((test_x, x[indx[b*10+500:(b+1)*10+500],:]), axis=0)\n",
    "                    test_y = np.concatenate((test_y, y[indx[b*10+500:(b+1)*10+500]]), axis=0)\n",
    "                \n",
    "            \n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp(data_x, data_y, class_idx, cv, ntrees=100, acorn=None):\n",
    "    if acorn is not None:\n",
    "        np.random.seed(acorn)\n",
    "        \n",
    "    train_x, train_y, test_x, test_y = cross_val_data(data_x, data_y, class_idx, cv=cv)\n",
    "        \n",
    "    m = 1000\n",
    "    errors = [[] for i in range(100)]\n",
    "    errors_1 = np.zeros(100,dtype=float)\n",
    "    \n",
    "    lifelong_forest = LifeLongForest()\n",
    "    \n",
    "    for ii in range(100):\n",
    "        lifelong_forest.new_forest(train_x[ii*500:(ii+1)*500,:], homogenize_labels(train_y[ii*500:(ii+1)*500]), n_estimators=ntrees)\n",
    "        #lifelong_forest.new_forest(train_x[(ii-1)*5000:(ii)*5000,:], homogenize_labels(train_y[(ii-1)*5000:(ii)*5000]), n_estimators=ntrees)\n",
    "        \n",
    "        task_id  = ii//10\n",
    "        llf_task=lifelong_forest.predict(test_x[task_id*1000:(task_id+1)*1000,:], representation=ii, decider=ii)\n",
    "        errors_1[ii] = 1 - np.sum(llf_task == homogenize_labels(test_y[task_id*1000:(task_id+1)*1000]))/m\n",
    "        \n",
    "        for jj in range(ii+1):\n",
    "            task_id  = jj//10\n",
    "            llf_task=lifelong_forest.predict(test_x[task_id*1000:(task_id+1)*1000,:], representation='all', decider=jj)\n",
    "            errors[ii].append(1 - np.sum(llf_task == homogenize_labels(test_y[task_id*1000:(task_id+1)*1000]))/m)\n",
    "            \n",
    "    with open('../result/'+'LF'+str(cv)+'.pickle', 'wb') as f:\n",
    "        pickle.dump(errors, f)\n",
    "        \n",
    "    with open('../result/'+'LF_single_task'+str(cv)+'.pickle', 'wb') as f:\n",
    "        pickle.dump(errors_1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tasks = 10\n",
    "train_file = '/data/Jayanta/continual-learning/train'\n",
    "unpickled_train = unpickle(train_file)\n",
    "train_keys = list(unpickled_train.keys())\n",
    "fine_labels = np.array(unpickled_train[train_keys[2]])\n",
    "labels = fine_labels\n",
    "\n",
    "test_file = '/data/Jayanta/continual-learning/test'\n",
    "unpickled_test = unpickle(test_file)\n",
    "test_keys = list(unpickled_test.keys())\n",
    "fine_labels = np.array(unpickled_test[test_keys[2]])\n",
    "labels_ = fine_labels\n",
    "\n",
    "data_x = np.concatenate((unpickled_train[b'data'], unpickled_test[b'data']), axis=0)\n",
    "data_y = np.concatenate((labels, labels_), axis=0)\n",
    "\n",
    "class_idx = [np.where(data_y == u)[0] for u in np.unique(data_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing 1 fold\n",
      "single training time 4.904431\n",
      "single prediction time 1.506881\n",
      "multitask prediction time 1.422830\n",
      "single training time 4.299433\n",
      "single prediction time 3.175934\n",
      "multitask prediction time 6.032343\n",
      "multitask prediction time 5.769995\n",
      "single training time 6.924241\n",
      "single prediction time 6.463519\n",
      "multitask prediction time 16.634370\n",
      "multitask prediction time 16.411392\n",
      "multitask prediction time 16.293101\n",
      "single training time 9.621274\n",
      "single prediction time 10.394316\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-0564256ee6d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Doing %d fold\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntrees\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macorn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"total time for one fold %f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mending\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-127193bf3b90>\u001b[0m in \u001b[0;36mexp\u001b[0;34m(data_x, data_y, class_idx, cv, ntrees, acorn)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mtask_id\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mjj\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mllf_task\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlifelong_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepresentation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecider\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multitask prediction time %f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mending\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-731152538b3e>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, representation, decider)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepresentation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecider\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mtask_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_across_tasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecider\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtask_classes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimate_posteriors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepresentation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecider\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-53-731152538b3e>\u001b[0m in \u001b[0;36m_estimate_posteriors\u001b[0;34m(self, X, representation, decider)\u001b[0m\n\u001b[1;32m    114\u001b[0m         posteriors_across_tasks = np.array(\n\u001b[1;32m    115\u001b[0m                     Parallel(n_jobs=-1)(\n\u001b[0;32m--> 116\u001b[0;31m                             \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer_task_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtransformer_task_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrepresentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                     )\n\u001b[1;32m    118\u001b[0m             )    \n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    538\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Parallel(n_jobs=-2,verbose=1)(\n",
    "#    delayed(exp)(\n",
    "#        data_x, data_y, class_idx, i, ntrees=100, acorn=i) for i in range(1,7,1)\n",
    "#)\n",
    "\n",
    "for i in range(1,7,1):\n",
    "    print(\"Doing %d fold\"%i)\n",
    "    exp(data_x, data_y, class_idx, i, ntrees=100, acorn=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
