{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "import pickle\n",
    "from itertools import product\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "#Infrastructure\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import NotFittedError\n",
    "\n",
    "#Data Handling\n",
    "from sklearn.utils.validation import (\n",
    "    check_X_y,\n",
    "    check_array,\n",
    "    NotFittedError,\n",
    ")\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "\n",
    "#Utils\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.base import clone \n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def homogenize_labels(a):\n",
    "    u = np.unique(a)\n",
    "    return np.array([np.where(u == i)[0][0] for i in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _finite_sample_correction(posteriors, num_points_in_partition, num_classes):\n",
    "    '''\n",
    "    encourage posteriors to approach uniform when there is low data\n",
    "    '''\n",
    "    correction_constant = 1 / (num_classes * num_points_in_partition)\n",
    "\n",
    "    zero_posterior_idxs = np.where(posteriors == 0)[0]\n",
    "\n",
    "    c = len(zero_posterior_idxs) / (num_classes * num_points_in_partition)\n",
    "    posteriors *= (1 - c)\n",
    "    posteriors[zero_posterior_idxs] = correction_constant\n",
    "    return posteriors\n",
    "\n",
    "class UncertaintyForest(BaseEstimator, ClassifierMixin):\n",
    "    '''\n",
    "    based off of https://arxiv.org/pdf/1907.00325.pdf\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_depth=30,\n",
    "        min_samples_leaf=1,\n",
    "        max_samples = 0.32,\n",
    "        max_features_tree = \"auto\",\n",
    "        n_estimators=200,\n",
    "        bootstrap=False,\n",
    "        parallel=True):\n",
    "\n",
    "        #Tree parameters.\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features_tree = max_features_tree\n",
    "\n",
    "        #Bag parameters\n",
    "        self.n_estimators = n_estimators\n",
    "        self.bootstrap = bootstrap\n",
    "        self.max_samples = max_samples\n",
    "\n",
    "        #Model parameters.\n",
    "        self.parallel = parallel\n",
    "        self.fitted = False\n",
    "\n",
    "    def _check_fit(self):\n",
    "        '''\n",
    "        raise a NotFittedError if the model isn't fit\n",
    "        '''\n",
    "        if not self.fitted:\n",
    "                msg = (\n",
    "                        \"This %(name)s instance is not fitted yet. Call 'fit' with \"\n",
    "                        \"appropriate arguments before using this estimator.\"\n",
    "                )\n",
    "                raise NotFittedError(msg % {\"name\": type(self).__name__})\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        get the estimated posteriors across trees\n",
    "        '''\n",
    "        X = check_array(X)\n",
    "                \n",
    "        def worker(tree_idx, tree):\n",
    "            #get the nodes of X\n",
    "            # Drop each estimation example down the tree, and record its 'y' value.\n",
    "            return tree.apply(X)\n",
    "            \n",
    "\n",
    "        if self.parallel:\n",
    "            return np.array(\n",
    "                    Parallel(n_jobs=-1)(\n",
    "                            delayed(worker)(tree_idx, tree) for tree_idx, tree in enumerate(self.ensemble.estimators_)\n",
    "                    )\n",
    "            )         \n",
    "        else:\n",
    "            return np.array(\n",
    "                    [worker(tree_idx, tree) for tree_idx, tree in enumerate(self.ensemble.estimators_)]\n",
    "                    )\n",
    "        \n",
    "    def get_transformer(self):\n",
    "        return lambda X : self.transform(X)\n",
    "        \n",
    "    def vote(self, nodes_across_trees):\n",
    "        return self.voter.predict(nodes_across_trees)\n",
    "        \n",
    "    def get_voter(self):\n",
    "        return self.voter\n",
    "        \n",
    "                        \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        #format X and y\n",
    "        X, y = check_X_y(X, y)\n",
    "        check_classification_targets(y)\n",
    "        self.classes_, y = np.unique(y, return_inverse=True)\n",
    "        \n",
    "        #define the ensemble\n",
    "        self.ensemble = BaggingClassifier(\n",
    "            DecisionTreeClassifier(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                max_features=self.max_features_tree\n",
    "            ),\n",
    "            n_estimators=self.n_estimators,\n",
    "            max_samples=self.max_samples,\n",
    "            bootstrap=self.bootstrap,\n",
    "            n_jobs = -1\n",
    "        )\n",
    "        \n",
    "        #fit the ensemble\n",
    "        self.ensemble.fit(X, y)\n",
    "        \n",
    "        class Voter(BaseEstimator):\n",
    "            def __init__(self, estimators_samples_, classes, parallel = True):\n",
    "                self.n_estimators = len(estimators_samples_)\n",
    "                self.classes_ = classes\n",
    "                self.parallel = parallel\n",
    "                self.estimators_samples_ = estimators_samples_\n",
    "            \n",
    "            def fit(self, nodes_across_trees, y, fitting = False):\n",
    "                self.tree_idx_to_node_ids_to_posterior_map = {}\n",
    "\n",
    "                def worker(tree_idx):\n",
    "                    nodes = nodes_across_trees[tree_idx]\n",
    "                    cal_nodes = nodes[self.estimators_samples_[tree_idx]] if fitting else nodes\n",
    "                    y_cal = y[self.estimators_samples_[tree_idx]] if fitting else y                    \n",
    "                    \n",
    "                    #create a map from the unique node ids to their classwise posteriors\n",
    "                    node_ids_to_posterior_map = {}\n",
    "\n",
    "                    #fill in the posteriors \n",
    "                    for node_id in np.unique(cal_nodes):\n",
    "                        cal_idxs_of_node_id = np.where(cal_nodes == node_id)[0]\n",
    "                        cal_ys_of_node = y_cal[cal_idxs_of_node_id]\n",
    "                        class_counts = [len(np.where(cal_ys_of_node == y)[0]) for y in np.unique(y) ]\n",
    "                        posteriors = np.nan_to_num(np.array(class_counts) / np.sum(class_counts))\n",
    "\n",
    "                        #finite sample correction\n",
    "                        posteriors_corrected = _finite_sample_correction(posteriors, len(cal_idxs_of_node_id), len(self.classes_))\n",
    "                        node_ids_to_posterior_map[node_id] = posteriors_corrected\n",
    "\n",
    "                    #add the node_ids_to_posterior_map to the overall tree_idx map \n",
    "                    self.tree_idx_to_node_ids_to_posterior_map[tree_idx] = node_ids_to_posterior_map\n",
    "                    \n",
    "                for tree_idx in range(self.n_estimators):\n",
    "                        worker(tree_idx)\n",
    "                return self\n",
    "                        \n",
    "                        \n",
    "            def predict_proba(self, nodes_across_trees):\n",
    "                def worker(tree_idx):\n",
    "                    #get the node_ids_to_posterior_map for this tree\n",
    "                    node_ids_to_posterior_map = self.tree_idx_to_node_ids_to_posterior_map[tree_idx]\n",
    "\n",
    "                    #get the nodes of X\n",
    "                    nodes = nodes_across_trees[tree_idx]\n",
    "\n",
    "                    posteriors = []\n",
    "                    node_ids = node_ids_to_posterior_map.keys()\n",
    "\n",
    "                    #loop over nodes of X\n",
    "                    for node in nodes:\n",
    "                        #if we've seen this node before, simply get the posterior\n",
    "                        if node in node_ids:\n",
    "                            posteriors.append(node_ids_to_posterior_map[node])\n",
    "                        #if we haven't seen this node before, simply use the uniform posterior \n",
    "                        else:\n",
    "                            posteriors.append(np.ones((len(np.unique(self.classes_)))) / len(self.classes_))\n",
    "                    return posteriors\n",
    "\n",
    "                if self.parallel:\n",
    "                    return np.mean(\n",
    "                            Parallel(n_jobs=-1)(\n",
    "                                    delayed(worker)(tree_idx) for tree_idx in range(self.n_estimators)\n",
    "                            ), axis = 0\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    return np.mean(\n",
    "                            [worker(tree_idx) for tree_idx in range(self.n_estimators)], axis = 0)\n",
    "                \n",
    "        #get the nodes of the calibration set\n",
    "        nodes_across_trees = self.transform(X) \n",
    "        self.voter = Voter(estimators_samples_ = self.ensemble.estimators_samples_, classes = self.classes_, parallel = self.parallel)\n",
    "        self.voter.fit(nodes_across_trees, y, fitting = True)\n",
    "        self.fitted = True\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.classes_[np.argmax(self.predict_proba(X), axis=-1)]\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.voter.predict_proba(self.transform(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LifeLongForest():\n",
    "    def __init__(self, acorn = None, verbose = False, model = \"uf\"):\n",
    "        self.X_across_tasks = []\n",
    "        self.y_across_tasks = []\n",
    "        \n",
    "        self.transformers_across_tasks = []\n",
    "        \n",
    "        #element [i, j] votes on decider from task i under representation from task j\n",
    "        self.voters_across_tasks_matrix = []\n",
    "        self.n_tasks = 0\n",
    "        \n",
    "        self.classes_across_tasks = []\n",
    "        \n",
    "        if acorn is not None:\n",
    "            np.random.seed(acorn)\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "    def check_task_idx_(self, task_idx):\n",
    "        if task_idx >= self.n_tasks:\n",
    "            raise Exception(\"Invalid Task IDX\")\n",
    "    \n",
    "    def new_forest(self, \n",
    "                   X, \n",
    "                   y, \n",
    "                   epochs = 100, \n",
    "                   lr = 5e-4, \n",
    "                   n_estimators = 10, \n",
    "                   max_samples = .63,\n",
    "                   bootstrap = True,\n",
    "                   max_depth = 30,\n",
    "                   min_samples_leaf = 1,\n",
    "                   acorn = None):\n",
    "        \n",
    "        #if self.model == \"dnn\":\n",
    "        #    from honest_dnn import HonestDNN \n",
    "        #if self.model == \"uf\":\n",
    "        #    from uncertainty_forest import UncertaintyForest\n",
    "        \n",
    "        self.X_across_tasks.append(X)\n",
    "        self.y_across_tasks.append(y)\n",
    "        \n",
    "        if self.model == \"dnn\":\n",
    "            new_honest_dnn = HonestDNN(verbose = self.verbose)\n",
    "            new_honest_dnn.fit(X, y, epochs = epochs, lr = lr)\n",
    "        if self.model == \"uf\":\n",
    "            new_honest_dnn = UncertaintyForest(n_estimators = n_estimators,\n",
    "                                               max_samples = max_samples,\n",
    "                                               bootstrap = bootstrap,\n",
    "                                               max_depth = max_depth,\n",
    "                                               min_samples_leaf = min_samples_leaf,\n",
    "                                               parallel = True)\n",
    "            new_honest_dnn.fit(X, y)\n",
    "        new_transformer = new_honest_dnn.get_transformer()\n",
    "        new_voter = new_honest_dnn.get_voter()\n",
    "        new_classes = new_honest_dnn.classes_\n",
    "        \n",
    "        self.transformers_across_tasks.append(new_transformer)\n",
    "        self.classes_across_tasks.append(new_classes)\n",
    "        \n",
    "        #add one voter to previous task voter lists under the new transformation\n",
    "        for task_idx in range(self.n_tasks):\n",
    "            X_of_task, y_of_task = self.X_across_tasks[task_idx], self.y_across_tasks[task_idx]\n",
    "            if self.model == \"dnn\":\n",
    "                X_of_task_under_new_transform = new_transformer.predict(X_of_task) \n",
    "            if self.model == \"uf\":\n",
    "                X_of_task_under_new_transform = new_transformer(X_of_task) \n",
    "            unfit_task_voter_under_new_transformation = clone(self.voters_across_tasks_matrix[task_idx][0])\n",
    "            if self.model == \"uf\":\n",
    "                unfit_task_voter_under_new_transformation.classes_ = self.voters_across_tasks_matrix[task_idx][0].classes_\n",
    "            task_voter_under_new_transformation = unfit_task_voter_under_new_transformation.fit(X_of_task_under_new_transform, y_of_task)\n",
    "\n",
    "            self.voters_across_tasks_matrix[task_idx].append(task_voter_under_new_transformation)\n",
    "            \n",
    "        #add n_tasks voters to new task voter list under previous transformations \n",
    "        new_voters_under_previous_task_transformation = []\n",
    "        for task_idx in range(self.n_tasks):\n",
    "            transformer_of_task = self.transformers_across_tasks[task_idx]\n",
    "            if self.model == \"dnn\":\n",
    "                X_under_task_transformation = transformer_of_task.predict(X)\n",
    "            if self.model == \"uf\":\n",
    "                X_under_task_transformation = transformer_of_task(X)\n",
    "            unfit_new_task_voter_under_task_transformation = clone(new_voter)\n",
    "            if self.model == \"uf\":\n",
    "                unfit_new_task_voter_under_task_transformation.classes_ = new_voter.classes_\n",
    "            new_task_voter_under_task_transformation = unfit_new_task_voter_under_task_transformation.fit(X_under_task_transformation, y)\n",
    "            new_voters_under_previous_task_transformation.append(new_task_voter_under_task_transformation)\n",
    "            \n",
    "        #make sure to add the voter of the new task under its own transformation\n",
    "        new_voters_under_previous_task_transformation.append(new_voter)\n",
    "        \n",
    "        self.voters_across_tasks_matrix.append(new_voters_under_previous_task_transformation)\n",
    "        \n",
    "        self.n_tasks += 1\n",
    "        \n",
    "    def _estimate_posteriors(self, X, representation = 0, decider = 0):\n",
    "        self.check_task_idx_(decider)\n",
    "        \n",
    "        if representation == \"all\":\n",
    "            representation = range(self.n_tasks)\n",
    "        elif isinstance(representation, int):\n",
    "            representation = np.array([representation])\n",
    "        \n",
    "        posteriors_across_tasks = []\n",
    "        for transformer_task_idx in representation:\n",
    "            transformer = self.transformers_across_tasks[transformer_task_idx]\n",
    "            voter = self.voters_across_tasks_matrix[decider][transformer_task_idx]\n",
    "            if self.model == \"dnn\":\n",
    "                posteriors_across_tasks.append(voter.predict_proba(transformer.predict(X)))\n",
    "            if self.model == \"uf\":\n",
    "                posteriors_across_tasks.append(voter.predict_proba(transformer(X)))\n",
    "        return np.mean(posteriors_across_tasks, axis = 0)\n",
    "    \n",
    "    def predict(self, X, representation = 0, decider = 0):\n",
    "        task_classes = self.classes_across_tasks[decider]\n",
    "        return task_classes[np.argmax(self._estimate_posteriors(X, representation, decider), axis = -1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_data(data_x, data_y, class_idx, total_cls=100, cv=1):\n",
    "    x = data_x.copy()\n",
    "    y = data_y.copy()\n",
    "    idx = class_idx.copy()\n",
    "    \n",
    "    \n",
    "    for i in range(total_cls):\n",
    "        indx = np.roll(idx[i],(cv-1)*100)\n",
    "        \n",
    "        if i==0:\n",
    "            #train_x = x[indx[0:500],:]\n",
    "            train_x = x[indx[0:500],:]\n",
    "            test_x = x[indx[500:600],:]\n",
    "            #train_y = y[indx[0:500]]\n",
    "            train_y = y[indx[0:500]]\n",
    "            test_y = y[indx[500:600]]\n",
    "        else:\n",
    "            #train_x = np.concatenate((train_x, x[indx[0:500],:]), axis=0)\n",
    "            train_x = np.concatenate((train_x, x[indx[0:500],:]), axis=0)\n",
    "            test_x = np.concatenate((test_x, x[indx[500:600],:]), axis=0)\n",
    "            #train_y = np.concatenate((train_y, y[indx[0:500]]), axis=0)\n",
    "            train_y = np.concatenate((train_y, y[indx[0:500]]), axis=0)\n",
    "            test_y = np.concatenate((test_y, y[indx[500:600]]), axis=0)\n",
    "        \n",
    "        \n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tasks = 10\n",
    "train_file = '/data/Jayanta/continual-learning/train'\n",
    "unpickled_train = unpickle(train_file)\n",
    "train_keys = list(unpickled_train.keys())\n",
    "fine_labels = np.array(unpickled_train[train_keys[2]])\n",
    "labels = fine_labels\n",
    "\n",
    "test_file = '/data/Jayanta/continual-learning/test'\n",
    "unpickled_test = unpickle(test_file)\n",
    "test_keys = list(unpickled_test.keys())\n",
    "fine_labels = np.array(unpickled_test[test_keys[2]])\n",
    "labels_ = fine_labels\n",
    "\n",
    "data_x = np.concatenate((unpickled_train[b'data'], unpickled_test[b'data']), axis=0)\n",
    "data_y = np.concatenate((labels, labels_), axis=0)\n",
    "\n",
    "class_idx = [np.where(data_y == u)[0] for u in np.unique(data_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp(train_x, train_y, test_x, test_y, task, n_data, ntrees=100, acorn=None):\n",
    "    \n",
    "    if acorn is not None:\n",
    "        np.random.seed(acorn)\n",
    "    \n",
    "    lf = LifeLongForest()\n",
    "    lf.new_forest(\n",
    "        train_x[task*5000:task*5000+n_data,:], homogenize_labels(train_y[task*5000:task*5000+n_data]),\n",
    "        n_estimators=ntrees\n",
    "    )\n",
    "    \n",
    "    lf_task=lf.predict(test_x[task*1000:(task+1)*1000,:], representation=0, decider=0)\n",
    "     \n",
    "    \n",
    "    return (1 - np.sum(lf_task == homogenize_labels(test_y[task*1000:(task+1)*1000]))/1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   31.1s remaining:  2.7min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   53.4s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   26.8s remaining:  2.3min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   49.5s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   27.5s remaining:  2.4min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   49.9s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   26.8s remaining:  2.3min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   48.3s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   26.4s remaining:  2.3min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   48.9s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   27.7s remaining:  2.4min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   48.8s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   21.8s remaining:  1.9min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   47.0s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   22.5s remaining:  2.0min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   47.3s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   20.4s remaining:  1.8min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   46.9s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   22.6s remaining:  2.0min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   46.8s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   22.4s remaining:  2.0min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   47.1s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   22.7s remaining:  2.0min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   46.6s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   24.5s remaining:  2.1min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   48.8s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   22.8s remaining:  2.0min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   49.1s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   22.6s remaining:  2.0min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   48.7s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   23.6s remaining:  2.1min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   48.0s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   23.5s remaining:  2.1min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   48.9s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   22.6s remaining:  2.0min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   47.6s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   24.8s remaining:  2.2min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   48.8s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   26.2s remaining:  2.3min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   47.7s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   24.5s remaining:  2.1min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   47.8s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   25.3s remaining:  2.2min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   48.0s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   25.5s remaining:  2.2min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   46.8s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   24.3s remaining:  2.1min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   47.9s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   25.6s remaining:  2.2min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   46.4s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   25.2s remaining:  2.2min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   46.2s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   25.9s remaining:  2.3min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   46.8s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   24.3s remaining:  2.1min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   46.3s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   23.6s remaining:  2.1min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   46.9s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   25.3s remaining:  2.2min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   46.4s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   21.7s remaining:  1.9min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   46.3s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   22.2s remaining:  1.9min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   45.8s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   20.3s remaining:  1.8min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   46.5s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   22.9s remaining:  2.0min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   45.8s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   22.4s remaining:  2.0min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   47.3s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   22.3s remaining:  1.9min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   45.8s finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   23.6s remaining:  2.1min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   46.9s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   22.6s remaining:  2.0min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   47.1s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   22.4s remaining:  2.0min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   48.0s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   22.5s remaining:  2.0min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   47.1s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   23.0s remaining:  2.0min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   46.9s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   22.3s remaining:  2.0min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   47.0s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   24.9s remaining:  2.2min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   47.6s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   26.7s remaining:  2.3min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   48.1s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   25.3s remaining:  2.2min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   48.5s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   25.9s remaining:  2.3min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   48.1s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   26.4s remaining:  2.3min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   47.9s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   26.0s remaining:  2.3min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   49.8s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   22.9s remaining:  2.0min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   48.4s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   25.2s remaining:  2.2min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   46.5s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   24.3s remaining:  2.1min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   47.2s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of  50 | elapsed:   24.3s remaining:  2.1min\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   47.0s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 47 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "n_data = np.arange(100,5100,100)\n",
    "\n",
    "for task in range(10):\n",
    "    error = np.zeros((len(n_data),6),dtype=float)\n",
    "    \n",
    "    for cv in range(1,7,1):\n",
    "        train_x, train_y, test_x, test_y = cross_val_data(data_x,data_y,class_idx,cv=cv)\n",
    "        error[:,cv-1] = Parallel(n_jobs=-2,verbose=1)(delayed(exp)(\n",
    "            train_x, train_y, test_x, test_y, task, n, ntrees=100, acorn=None\n",
    "        ) for n in n_data\n",
    "        )\n",
    "    \n",
    "    with open('../result/UF_'+str(task+1)+'.pickle','wb') as f:\n",
    "        pickle.dump(error,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
