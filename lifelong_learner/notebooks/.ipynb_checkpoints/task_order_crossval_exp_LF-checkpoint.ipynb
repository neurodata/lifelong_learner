{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from skimage.transform import rotate\n",
    "from scipy import ndimage\n",
    "from skimage.util import img_as_ubyte\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.ensemble.forest import _generate_unsampled_indices\n",
    "from sklearn.ensemble.forest import _generate_sample_indices\n",
    "import numpy as np\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from itertools import product\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def homogenize_labels(a):\n",
    "    u = np.unique(a)\n",
    "    return np.array([np.where(u == i)[0][0] for i in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LifelongForest:\n",
    "    \"\"\"\n",
    "    Lifelong Forest class.\n",
    "    \"\"\"\n",
    "    def __init__(self, acorn=None):\n",
    "        \"\"\"\n",
    "        Two major things the Forest Class needs access to:\n",
    "            1) the realized random forest model (self.models_ is a list of forests, 1 for each task)\n",
    "            2) old data (to update posteriors when a new task is introduced)\n",
    "        \"\"\"\n",
    "        self.models_ = []\n",
    "        self.X_ = []\n",
    "        self.y_ = []\n",
    "        self.n_tasks = 0\n",
    "        self.n_classes = None\n",
    "        if acorn is not None:\n",
    "            np.random.seed(acorn)\n",
    "    def new_forest(self, X, y, n_estimators=200, max_samples=0.32,\n",
    "                        bootstrap=True, max_depth=30, min_samples_leaf=1,\n",
    "                        acorn=None):\n",
    "        \"\"\"\n",
    "        Input\n",
    "        X: an array-like object of features; X.shape == (n_samples, n_features)\n",
    "        y: an array-like object of class labels; len(y) == n_samples\n",
    "        n_estimators: int; number of trees to construct (default = 200)\n",
    "        max_samples: float in (0, 1]: number of samples to consider when \n",
    "            constructing a new tree (default = 0.32)\n",
    "        bootstrap: bool; If True then the samples are sampled with replacement\n",
    "        max_depth: int; maximum depth of a tree\n",
    "        min_samples_leaf: int; minimum number of samples in a leaf node\n",
    "        Return\n",
    "        model: a BaggingClassifier fit to X, y\n",
    "        \"\"\"\n",
    "        if X.ndim == 1:\n",
    "            raise ValueError('1d data will cause headaches down the road')\n",
    "        if acorn is not None:\n",
    "            np.random.seed(acorn)\n",
    "        self.X_.append(X)\n",
    "        self.y_.append(y)\n",
    "        n = X.shape[0]\n",
    "        K = len(np.unique(y))\n",
    "        if self.n_classes is None:\n",
    "            self.n_classes = K\n",
    "        max_features = int(np.ceil(np.sqrt(X.shape[1])))\n",
    "        model=BaggingClassifier(DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf,\n",
    "                                                         max_features = max_features),\n",
    "                                  n_estimators=n_estimators,\n",
    "                                  max_samples=max_samples,\n",
    "                                  bootstrap=bootstrap)\n",
    "        model.fit(X, y)\n",
    "        self.models_.append(model)\n",
    "        self.n_tasks += 1\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        return model\n",
    "    def _get_leaves(self, estimator):\n",
    "        \"\"\"\n",
    "        Internal function to get leaf node ids of estimator.\n",
    "        Input\n",
    "        estimator: a fit DecisionTreeClassifier\n",
    "        Return\n",
    "        leaf_ids: numpy array; an array of leaf node ids\n",
    "        Usage\n",
    "        _estimate_posteriors(..)\n",
    "        \"\"\"\n",
    "        # adapted from https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\n",
    "        n_nodes = estimator.tree_.node_count\n",
    "        children_left = estimator.tree_.children_left\n",
    "        children_right = estimator.tree_.children_right\n",
    "        feature = estimator.tree_.feature\n",
    "        threshold = estimator.tree_.threshold\n",
    "        leaf_ids = []\n",
    "        stack = [(0, -1)] \n",
    "        while len(stack) > 0:\n",
    "            node_id, parent_depth = stack.pop()\n",
    "            # If we have a test node\n",
    "            if (children_left[node_id] != children_right[node_id]):\n",
    "                stack.append((children_left[node_id], parent_depth + 1))\n",
    "                stack.append((children_right[node_id], parent_depth + 1))\n",
    "            else:\n",
    "                leaf_ids.append(node_id)\n",
    "        return np.array(leaf_ids)\n",
    "    def _finite_sample_correction(self, class_probs, row_sums):\n",
    "        \"\"\"\n",
    "        An internal function for finite sample correction of posterior estimation.\n",
    "        Input\n",
    "        class_probs: numpy array; array of posteriors to correct\n",
    "        row_sums: numpy array; array of partition counts\n",
    "        Output\n",
    "        class_probs: numpy array; finite sample corrected posteriors\n",
    "        Usage\n",
    "        _estimate_posteriors(..)\n",
    "        \"\"\"\n",
    "        where_0 = np.argwhere(class_probs == 0)\n",
    "        for elem in where_0:\n",
    "            class_probs[elem[0], elem[1]] = 1 / (2 * row_sums[elem[0], None])\n",
    "        where_1 = np.argwhere(class_probs == 1)\n",
    "        for elem in where_1:\n",
    "            class_probs[elem[0], elem[1]] = 1 - 1 / (2 * row_sums[elem[0], None])\n",
    "        return class_probs\n",
    "    def _estimate_posteriors(self, test, representation=0, decider=0, subsample=1, acorn=None):\n",
    "        \"\"\"\n",
    "        An internal function to estimate the posteriors.\n",
    "        Input\n",
    "        task_number: int; indicates which model in self.model_ to use\n",
    "        test: array-like; test observation\n",
    "        in_task: bool; True if test is an in-task observation(s)\n",
    "        subsample: float in (0, 1]; proportion of out-of-task samples to use to\n",
    "            estimate posteriors\n",
    "        Return\n",
    "        probs: numpy array; probs[i, k] is the probability of observation i\n",
    "            being class k\n",
    "        Usage\n",
    "        predict(..)\n",
    "        \"\"\"\n",
    "        if acorn is not None:\n",
    "            acorn = np.random.seed(acorn)\n",
    "        if representation==decider:\n",
    "            in_task=True\n",
    "        else:\n",
    "            in_task=False\n",
    "        train = self.X_[decider]\n",
    "        y = self.y_[decider]\n",
    "        model = self.models_[representation]\n",
    "        n, d = train.shape\n",
    "        if test.ndim > 1:\n",
    "            m, d_ = test.shape\n",
    "        else:\n",
    "            m = len(test)\n",
    "            d_ = 1\n",
    "        size = len(np.unique(y))\n",
    "        class_counts = np.zeros((m, size))\n",
    "        for tree in model:\n",
    "            # get out of bag indicies\n",
    "            if in_task:\n",
    "                prob_indices = _generate_unsampled_indices(tree.random_state, n)\n",
    "                # in_bag_idx = _generate_sample_indices(tree.random_state, n) # this is not behaving as i expected\n",
    "            else:\n",
    "                prob_indices = np.random.choice(range(n), size=int(subsample*n), replace=False)\n",
    "            leaf_nodes = self._get_leaves(tree)\n",
    "            unique_leaf_nodes = np.unique(leaf_nodes)\n",
    "            # get all node counts\n",
    "            node_counts = tree.tree_.n_node_samples\n",
    "            # get probs for eval samples\n",
    "            posterior_class_counts = np.zeros((len(unique_leaf_nodes), size))\n",
    "            for prob_index in prob_indices:\n",
    "                temp_node = tree.apply(train[prob_index].reshape(1, -1)).item()\n",
    "                #print(y[prob_index], size, np.unique(y))\n",
    "                posterior_class_counts[np.where(unique_leaf_nodes == temp_node)[0][0], y[prob_index]] += 1\n",
    "            # total number of points in a node\n",
    "            row_sums = posterior_class_counts.sum(axis=1)\n",
    "            # no divide by zero\n",
    "            row_sums[row_sums == 0] = 1\n",
    "            # posteriors\n",
    "            class_probs = (posterior_class_counts / row_sums[:, None])\n",
    "            # posteriors with finite sampling correction\n",
    "            class_probs = self._finite_sample_correction(class_probs, row_sums)\n",
    "            # posteriors as a list\n",
    "            class_probs.tolist()\n",
    "            partition_counts = np.asarray([node_counts[np.where(unique_leaf_nodes == x)[0][0]] for x in tree.apply(test)])\n",
    "            # get probability for out of bag samples\n",
    "            eval_class_probs = [class_probs[np.where(unique_leaf_nodes == x)[0][0]] for x in tree.apply(test)]\n",
    "            eval_class_probs = np.array(eval_class_probs)\n",
    "            # find total elements for out of bag samples\n",
    "            elems = np.multiply(eval_class_probs, partition_counts[:, np.newaxis])\n",
    "            # store counts for each x (repeat fhis for each tree)\n",
    "            class_counts += elems\n",
    "        # calculate p(y|X = x) for all x's\n",
    "        probs = class_counts / class_counts.sum(axis=1, keepdims=True)\n",
    "        return probs\n",
    "    def predict(self, test, representation=0, decider='all', subsample=1, acorn=None):\n",
    "        \"\"\"\n",
    "        Predicts the class labels for each sample in test.\n",
    "        Input\n",
    "        test: array-like; either a 1d array of length n_features\n",
    "            or a 2d array of shape (m, n_features) \n",
    "        task_number: int; task number \n",
    "        \"\"\"\n",
    "        size=len(np.unique(self.y_[decider]))\n",
    "        sum_posteriors = np.zeros((test.shape[0], size))\n",
    "        if representation is 'all':\n",
    "            for i in range(self.n_tasks):\n",
    "                sum_posteriors += self._estimate_posteriors(test,\n",
    "                                                            i,\n",
    "                                                            decider,\n",
    "                                                            subsample,\n",
    "                                                            acorn)\n",
    "        else:\n",
    "            sum_posteriors += self._estimate_posteriors(test,\n",
    "                                                        representation,\n",
    "                                                        decider,\n",
    "                                                        subsample,\n",
    "                                                        acorn)\n",
    "        return np.argmax(sum_posteriors, axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class LifeLongDNN():\n",
    "    def __init__(self, acorn = None, verbose = False, model = \"uf\"):\n",
    "        self.X_across_tasks = []\n",
    "        self.y_across_tasks = []\n",
    "        \n",
    "        self.transformers_across_tasks = []\n",
    "        \n",
    "        #element [i, j] votes on decider from task i under representation from task j\n",
    "        self.voters_across_tasks_matrix = []\n",
    "        self.n_tasks = 0\n",
    "        \n",
    "        self.classes_across_tasks = []\n",
    "        \n",
    "        if acorn is not None:\n",
    "            np.random.seed(acorn)\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "    def check_task_idx_(self, task_idx):\n",
    "        if task_idx >= self.n_tasks:\n",
    "            raise Exception(\"Invalid Task IDX\")\n",
    "    \n",
    "    def new_forest(self, \n",
    "                   X, \n",
    "                   y, \n",
    "                   epochs = 100, \n",
    "                   lr = 5e-4, \n",
    "                   n_estimators = 100, \n",
    "                   max_samples = .32,\n",
    "                   bootstrap = True,\n",
    "                   max_depth = 30,\n",
    "                   min_samples_leaf = 1,\n",
    "                   acorn = None):\n",
    "        \n",
    "        if self.model == \"dnn\":\n",
    "            from honest_dnn import HonestDNN \n",
    "        if self.model == \"uf\":\n",
    "            from uncertainty_forest import UncertaintyForest\n",
    "        \n",
    "        self.X_across_tasks.append(X)\n",
    "        self.y_across_tasks.append(y)\n",
    "        \n",
    "        if self.model == \"dnn\":\n",
    "            new_honest_dnn = HonestDNN(verbose = self.verbose)\n",
    "            new_honest_dnn.fit(X, y, epochs = epochs, lr = lr)\n",
    "        if self.model == \"uf\":\n",
    "            new_honest_dnn = UncertaintyForest(n_estimators = n_estimators,\n",
    "                                               max_samples = max_samples,\n",
    "                                               bootstrap = bootstrap,\n",
    "                                               max_depth = max_depth,\n",
    "                                               min_samples_leaf = min_samples_leaf,\n",
    "                                               parallel = True)\n",
    "            new_honest_dnn.fit(X, y)\n",
    "        new_transformer = new_honest_dnn.get_transformer()\n",
    "        new_voter = new_honest_dnn.get_voter()\n",
    "        new_classes = new_honest_dnn.classes_\n",
    "        \n",
    "        self.transformers_across_tasks.append(new_transformer)\n",
    "        self.classes_across_tasks.append(new_classes)\n",
    "        \n",
    "        #add one voter to previous task voter lists under the new transformation\n",
    "        for task_idx in range(self.n_tasks):\n",
    "            X_of_task, y_of_task = self.X_across_tasks[task_idx], self.y_across_tasks[task_idx]\n",
    "            if self.model == \"dnn\":\n",
    "                X_of_task_under_new_transform = new_transformer.predict(X_of_task) \n",
    "            if self.model == \"uf\":\n",
    "                X_of_task_under_new_transform = new_transformer(X_of_task) \n",
    "            unfit_task_voter_under_new_transformation = clone(self.voters_across_tasks_matrix[task_idx][0])\n",
    "            if self.model == \"uf\":\n",
    "                unfit_task_voter_under_new_transformation.classes_ = self.voters_across_tasks_matrix[task_idx][0].classes_\n",
    "            task_voter_under_new_transformation = unfit_task_voter_under_new_transformation.fit(X_of_task_under_new_transform, y_of_task)\n",
    "\n",
    "            self.voters_across_tasks_matrix[task_idx].append(task_voter_under_new_transformation)\n",
    "            \n",
    "        #add n_tasks voters to new task voter list under previous transformations \n",
    "        new_voters_under_previous_task_transformation = []\n",
    "        for task_idx in range(self.n_tasks):\n",
    "            transformer_of_task = self.transformers_across_tasks[task_idx]\n",
    "            if self.model == \"dnn\":\n",
    "                X_under_task_transformation = transformer_of_task.predict(X)\n",
    "            if self.model == \"uf\":\n",
    "                X_under_task_transformation = transformer_of_task(X)\n",
    "            unfit_new_task_voter_under_task_transformation = clone(new_voter)\n",
    "            if self.model == \"uf\":\n",
    "                unfit_new_task_voter_under_task_transformation.classes_ = new_voter.classes_\n",
    "            new_task_voter_under_task_transformation = unfit_new_task_voter_under_task_transformation.fit(X_under_task_transformation, y)\n",
    "            new_voters_under_previous_task_transformation.append(new_task_voter_under_task_transformation)\n",
    "            \n",
    "        #make sure to add the voter of the new task under its own transformation\n",
    "        new_voters_under_previous_task_transformation.append(new_voter)\n",
    "        \n",
    "        self.voters_across_tasks_matrix.append(new_voters_under_previous_task_transformation)\n",
    "        \n",
    "        self.n_tasks += 1\n",
    "        \n",
    "    def _estimate_posteriors(self, X, representation = 0, decider = 0):\n",
    "        self.check_task_idx_(decider)\n",
    "        \n",
    "        if representation == \"all\":\n",
    "            representation = range(self.n_tasks)\n",
    "        elif isinstance(representation, int):\n",
    "            representation = np.array([representation])\n",
    "        \n",
    "        posteriors_across_tasks = []\n",
    "        for transformer_task_idx in representation:\n",
    "            transformer = self.transformers_across_tasks[transformer_task_idx]\n",
    "            voter = self.voters_across_tasks_matrix[decider][transformer_task_idx]\n",
    "            if self.model == \"dnn\":\n",
    "                posteriors_across_tasks.append(voter.predict_proba(transformer.predict(X)))\n",
    "            if self.model == \"uf\":\n",
    "                posteriors_across_tasks.append(voter.predict_proba(transformer(X)))\n",
    "        return np.mean(posteriors_across_tasks, axis = 0)\n",
    "    \n",
    "    def predict(self, X, representation = 0, decider = 0):\n",
    "        task_classes = self.classes_across_tasks[decider]\n",
    "        return task_classes[np.argmax(self._estimate_posteriors(X, representation, decider), axis = -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LF_experiment(train_x, train_y, test_x, test_y, cv, ntrees=41, acorn=None):\n",
    "    if acorn is not None:\n",
    "        np.random.seed(acorn)\n",
    "       \n",
    "    m = 1000\n",
    "    errors = [[] for i in range(10)]\n",
    "    \n",
    "    lifelong_forest = LifeLongDNN()\n",
    "    \n",
    "    for ii in range(10):\n",
    "        lifelong_forest.new_forest(train_x[ii*5000:(ii+1)*5000,:], homogenize_labels(train_y[ii*5000:(ii+1)*5000]), n_estimators=ntrees)\n",
    "        \n",
    "        for jj in range(ii+1):\n",
    "            llf_task=lifelong_forest.predict(test_x[jj*1000:(jj+1)*1000,:], representation='all', decider=jj)\n",
    "            errors[ii].append(1 - np.sum(llf_task == homogenize_labels(test_y[jj*1000:(jj+1)*1000]))/m)\n",
    "    \n",
    "    \n",
    "    \n",
    "    with open('/data/Jayanta/continual-learning/order_res/'+'LF'+'__'+str(cv)+'.pickle', 'wb') as f:\n",
    "        pickle.dump(errors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_data(data_x, data_y, class_idx, total_cls=100, cv=1):\n",
    "    x = data_x.copy()\n",
    "    y = data_y.copy()\n",
    "    idx = class_idx.copy()\n",
    "    \n",
    "    \n",
    "    for i in range(total_cls):\n",
    "        indx = np.roll(idx[i],(cv-1)*100)\n",
    "        \n",
    "        if i==0:\n",
    "            train_x = x[indx[0:500],:]\n",
    "            test_x = x[indx[500:600],:]\n",
    "            train_y = y[indx[0:500]]\n",
    "            test_y = y[indx[500:600]]\n",
    "        else:\n",
    "            train_x = np.concatenate((train_x, x[indx[0:500],:]), axis=0)\n",
    "            test_x = np.concatenate((test_x, x[indx[500:600],:]), axis=0)\n",
    "            train_y = np.concatenate((train_y, y[indx[0:500]]), axis=0)\n",
    "            test_y = np.concatenate((test_y, y[indx[500:600]]), axis=0)\n",
    "        \n",
    "        \n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def sort_data(data_x, data_y, class_idx, total_cls=100):\n",
    "    x = data_x.copy()\n",
    "    y = data_y.copy()\n",
    "    idx = class_idx.copy()\n",
    "    \n",
    "    \n",
    "    for i in range(total_cls):\n",
    "        indx = idx[i]\n",
    "        \n",
    "        if i==0:\n",
    "            train_x = x[indx[0:500],:]\n",
    "            test_x = x[indx[500:600],:]\n",
    "            train_y = y[indx[0:500]]\n",
    "            test_y = y[indx[500:600]]\n",
    "        else:\n",
    "            train_x = np.concatenate((train_x, x[indx[0:500],:]), axis=0)\n",
    "            test_x = np.concatenate((test_x, x[indx[500:600],:]), axis=0)\n",
    "            train_y = np.concatenate((train_y, y[indx[0:500]]), axis=0)\n",
    "            test_y = np.concatenate((test_y, y[indx[500:600]]), axis=0)\n",
    "        \n",
    "        \n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "#%%\n",
    "def change_labels(labels):\n",
    "    lbl = labels.copy()\n",
    "    l = len(lbl)\n",
    "    for i in range(l):\n",
    "        lbl[i] = np.mod(lbl[i]+10,100)\n",
    "\n",
    "    return lbl\n",
    "    \n",
    "#%%\n",
    "def LF_experiment(train_x, train_y, test_x, test_y, order, ntrees=41, cv=1, acorn=None):\n",
    "    if acorn is not None:\n",
    "        np.random.seed(acorn)\n",
    "       \n",
    "    m = 1000\n",
    "    errors = [[] for i in range(10)]\n",
    "    \n",
    "    lifelong_forest = LifelongForest()\n",
    "    \n",
    "    for ii in range(10):\n",
    "        lifelong_forest.new_forest(train_x[ii*5000:(ii+1)*5000,:], homogenize_labels(train_y[ii*5000:(ii+1)*5000]), n_estimators=ntrees)\n",
    "        \n",
    "        for jj in range(ii+1):\n",
    "            llf_task=lifelong_forest.predict(test_x[jj*1000:(jj+1)*1000,:], representation='all', decider=jj)\n",
    "            errors[ii].append(1 - np.sum(llf_task == homogenize_labels(test_y[jj*1000:(jj+1)*1000]))/m)\n",
    "            print(np.sum(llf_task == homogenize_labels(test_y[jj*1000:(jj+1)*1000]))/m)\n",
    "            #print(llf_task,homogenize_labels(test_y[jj*1000:(jj+1)*1000]))\n",
    "    \n",
    "\n",
    "    with open('/data/Jayanta/continual-learning/order_res/'+'LF'+str(order)+'_'+str(cv)+'.pickle', 'wb') as f:\n",
    "        pickle.dump(errors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp(order,cv):\n",
    "    n_tasks = 10\n",
    "    train_file = './cifar-100-python/train'\n",
    "    unpickled_train = unpickle(train_file)\n",
    "    train_keys = list(unpickled_train.keys())\n",
    "    fine_labels = np.array(unpickled_train[train_keys[2]])\n",
    "    labels = fine_labels\n",
    "\n",
    "    test_file = './cifar-100-python/test'\n",
    "    unpickled_test = unpickle(test_file)\n",
    "    test_keys = list(unpickled_test.keys())\n",
    "    fine_labels = np.array(unpickled_test[test_keys[2]])\n",
    "    labels_ = fine_labels\n",
    "\n",
    "\n",
    "    data_x = np.concatenate((unpickled_train[b'data'], unpickled_test[b'data']), axis=0)\n",
    "\n",
    "\n",
    "    for i in range(order):\n",
    "        labels = change_labels(labels)\n",
    "        labels_ = change_labels(labels_)\n",
    "    \n",
    "        #data_x = np.concatenate((unpickled_train[b'data'], unpickled_test[b'data']), axis=0)\n",
    "    data_y = np.concatenate((labels, labels_), axis=0)\n",
    "\n",
    "    class_idx = [np.where(data_y == u)[0] for u in np.unique(data_y)]\n",
    "    \n",
    "    train_x, train_y, test_x, test_y = cross_val_data(data_x,data_y,class_idx,cv=cv+1)\n",
    "    LF_experiment(train_x, train_y, test_x, test_y, order=order, cv=cv, acorn=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 out of  60 | elapsed:    1.8s remaining:    2.3s\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './cifar-100-python/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/jayanta/env/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"/home/jayanta/env/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/home/jayanta/env/lib/python3.6/site-packages/joblib/_parallel_backends.py\", line 567, in __call__\n    return self.func(*args, **kwargs)\n  File \"/home/jayanta/env/lib/python3.6/site-packages/joblib/parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"/home/jayanta/env/lib/python3.6/site-packages/joblib/parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"<ipython-input-13-efdc6c37fc72>\", line 4, in exp\n  File \"<ipython-input-8-247960ee1c98>\", line 2, in unpickle\nFileNotFoundError: [Errno 2] No such file or directory: './cifar-100-python/train'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b6e9b39aa38a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0miterable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mordr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './cifar-100-python/train'"
     ]
    }
   ],
   "source": [
    "ordr = range(1,11,1)\n",
    "cv_ = range(6)\n",
    "\n",
    "iterable = product(ordr,cv_)\n",
    "\n",
    "Parallel(n_jobs=-1,verbose=1)(delayed(exp)(i,cv) for i,cv in iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
